{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bedcc3-21db-42b8-b8f9-879b4d85eb41",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is intended to process wikipedia dumps, with the goal of extract the maximum number of complete and significative sentences. This set of sentences could be used for any purpose in [NLP](https://en.wikipedia.org/wiki/Natural_language_processing), but information is also extracted which would be meaningful for _wikipedia_ itself.\n",
    "\n",
    "The developed example is about _Galipedia_, the galician wikipedia, but it could be easily adapted for other languages just because it is _language-agnostic_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71251b-107e-4ce1-8957-7609d244bac6",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "From __[Wikimedia Downloads](https://dumps.wikimedia.org/mirrors.html)__\n",
    "\n",
    "from the mirror _Academic Computer Club, Umeå University_ (Last 5 good XML dumps, 'other' datasets): glwiki-20221120-pages-articles.xml.bz2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603ac9db-0750-4bde-bfa2-1ae076373ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-08 22:20:53--  http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
      "Resolving ftp.acc.umu.se (ftp.acc.umu.se)... 194.71.11.173, 194.71.11.165, 194.71.11.163, ...\n",
      "Connecting to ftp.acc.umu.se (ftp.acc.umu.se)|194.71.11.173|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://gemmei.ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2 [following]\n",
      "--2022-12-08 22:20:54--  http://gemmei.ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
      "Resolving gemmei.ftp.acc.umu.se (gemmei.ftp.acc.umu.se)... 194.71.11.137, 2001:6b0:19::137\n",
      "Connecting to gemmei.ftp.acc.umu.se (gemmei.ftp.acc.umu.se)|194.71.11.137|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 302586132 (289M) [application/x-bzip2]\n",
      "Saving to: ‘glwiki-20221120-pages-articles.xml.bz2’\n",
      "\n",
      "glwiki-20221120-pag 100%[===================>] 288.57M  36.7MB/s    in 8.5s    \n",
      "\n",
      "2022-12-08 22:21:02 (34.0 MB/s) - ‘glwiki-20221120-pages-articles.xml.bz2’ saved [302586132/302586132]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
    "\n",
    "\n",
    "!bunzip2 glwiki-20221120-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbabb5-273e-4969-b2f5-2d8cf67ee043",
   "metadata": {},
   "source": [
    "# Libraries & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb79d118-c6d7-40ff-ae18-bc09973fe4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os,pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from random import choice, sample\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "document=namedtuple('document',['title','category','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e26016-350a-435a-8758-fb3988ff079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(lst):\n",
    "    '''Unravel a list of lists\n",
    "    lst: a list or set or tuple of lists/sets/tuples\n",
    "    returns all values in one list'''\n",
    "    ulst=[]\n",
    "    for item in lst:\n",
    "        if type(item) in [list,set,tuple]:\n",
    "            ulst+=unravel(item)\n",
    "        else:\n",
    "            ulst.append(item)\n",
    "    return ulst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d128e365-e5a7-4eeb-b209-0724a1c3fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorice(iterable_list,thread_function,max_workers=None):\n",
    "    '''\n",
    "    given an iterable list with data, and a thread_function for process it\n",
    "    set up a Pool for vectorice the function and returns the bunches in answer.\n",
    "    max_workers stands for the number of threads launched. \n",
    "    If max_workers is None (or not an integer) it is set to the number of CPUs detected\n",
    "    '''\n",
    "    from multiprocessing.pool import Pool\n",
    "    \n",
    "    max_workers=max_workers if isinstance(max_workers,int) else os.cpu_count()\n",
    "    \n",
    "    R=lambda x,y=max_workers: list(range(0,len(x),len(x)//y))\n",
    "    \n",
    "    lR=R(iterable_list)\n",
    "    \n",
    "    params=[iterable_list[lR[i]:lR[i+1]] for i in range(len(lR)-2)]\n",
    "    params.append(iterable_list[lR[-2]:])\n",
    "    \n",
    "    pool=Pool()\n",
    "    answer=pool.map(thread_function,params)\n",
    "    del pool\n",
    "\n",
    "    return answer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e382aec-f2b0-42fa-9bc9-3b4f585afa06",
   "metadata": {},
   "source": [
    "It could be difficult preserve the _language-agnostic_ nature for sentence tokenization. Two ways are plausible:\n",
    "* import a sentence tokenizer from nltk or any other suitable library\n",
    "* write a custom function, which can take into account especifities of the dump we work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00731fa2-ef8f-4d21-a6a4-40fad2d113d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import nltk\n",
    "#sent_tok= nltk.sent_tokenize\n",
    "\n",
    "def sent_tok(text,ends='[\\:\\?\\!\\*\\#]'):\n",
    "    if type(text)!=list:\n",
    "        text=[text]\n",
    "    res=[]\n",
    "    for item in text:\n",
    "        #preserve ellipsis\n",
    "        item=item.replace('...','…')\n",
    "        #preserve some common abreviatures\n",
    "        item=item.replace('a.C.','aC.').replace('d.C.','dC.')\n",
    "        #preserve acronyms\n",
    "        ini0=0\n",
    "        sent=''\n",
    "        for span in re.finditer('[^A-Zªº]\\. ',item):\n",
    "            ini,fin=span.span()\n",
    "            sent+=item[ini0:ini+1]+'.\\n'\n",
    "            ini0=fin\n",
    "        \n",
    "        res.append(re.sub(ends,'\\n',sent).split('\\n'))\n",
    "       \n",
    "    return ([item.strip() for item in unravel(res) if item.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16312b78-c976-4e13-be46-357343854093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_in(txt,pttrn=[':','.jpg','.png','*[']):\n",
    "    '''returns True if any of pttr is in txt'''\n",
    "    for pt in pttrn:\n",
    "        if pt in txt:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bb626-fb2d-45ac-b979-35e1b3ad8221",
   "metadata": {},
   "source": [
    "The function `get_links` try to get information to remove some html patterns which could be recursive and with unbalanced open and close tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a053825-ef3f-4b29-8bcd-bf285618f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page,pat_open=r'\\[\\[',pat_close=r'\\]\\]'):\n",
    "    \n",
    "   \n",
    "    \n",
    "    lini=[item.span() for item in re.finditer(pat_open,page)]\n",
    "    lfin=[item.span() for item in re.finditer(pat_close,page)]\n",
    "    if len(lfin)==0 and len(lini):\n",
    "        ini=lini[0][0] if len(lini) else 0\n",
    "        fin=len(page)-1\n",
    "        return [page[ini:fin]]\n",
    "    \n",
    "    chunks=[]\n",
    "    if len(lini)!=len(lfin):\n",
    "\n",
    "\n",
    "        indxf=0\n",
    "        indxi=0\n",
    "        while indxf<len(lfin) and indxi<len(lini):\n",
    "            ini=lini[indxi][0]\n",
    "            while indxi<len(lini) and lini[indxi][1]<lfin[indxf][0] :\n",
    "                indxi+=1\n",
    "            if indxi==len(lini):\n",
    "                fin=lfin[-1][1]\n",
    "            else:\n",
    "                while indxf<len(lfin) and lfin[indxf][0]<lini[indxi][1]:\n",
    "                    indxf+=1\n",
    "                fin=lfin[indxf-1][1]\n",
    "            chunks.append(page[ini:fin])\n",
    "    else:\n",
    "        posini=[]\n",
    "        for indx,posfin in enumerate(lfin):\n",
    "            posini+=[item for item in lini[indx:] if item[1]<posfin[0]]\n",
    "            if len([item for item in lini[indx:] if item[1]<posfin[0]])==1:\n",
    "                chunks.append(page[posini[0][0]:posfin[1]])\n",
    "                posini=[]\n",
    "            \n",
    "    \n",
    "    return sorted(chunks,key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b15820-5ab7-4567-90f2-44b93182d145",
   "metadata": {},
   "source": [
    "## Cleaning pages\n",
    "\n",
    "The function `clean_page` is the core of this notebook. This function accepts a wiki page and process it. It returns 4 elements:\n",
    "* `title`: page title, string\n",
    "* `contributors`: list with creator username\n",
    "* `category`: list with assigned categories\n",
    "* `text`: clean text, cleaned as described below  \n",
    "\n",
    "There are a number of language dependent patterns, in this notebook the  selected patterns work with _Galipedia_, but must be easy adapt them for other languages:\n",
    "* `patt_category`: pattern to extract categories\n",
    "* `pages_to_drop`: patterns to identify in the title internal pages of wikipedia , such as 'Help', 'Model', ..., which do not contains any significant text.\n",
    "* `terminal_sections`: The wiki articles have a defined structure and there are sections located at the end of articles whithout any significant text, like 'Bibliography', 'Notes',... \n",
    "* `patt_citation`: pattern for extract textual citations and incorporate it into text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165120b2-7775-4190-a84a-f42c793d368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patt_category=r'\\[\\[Categoría:(.*?)\\]\\]'\n",
    "pages_todrop=['Axuda:', 'Wikipedia:','MediaWiki:','Modelo:','Categoría:','Módulo:']\n",
    "terminal_sections= ['Palmarés',  'Festividades',  'Partidos históricos.*?',  'Filmografía',  'Galería.*?',  'Notas',  'Véxase tamén',  \n",
    "                 'Bibliografía',  'Outros artigos',  'Ligazóns externas']\n",
    "patt_citation=r'\\{\\{cita ?\\|(.*?\\.)\\}\\}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049a68ad-939d-458f-a6b0-129083d275cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page(page):\n",
    "    title=re.findall(r'<title>(.*?)</title>',page)\n",
    "    contributors=re.findall(r'<username>(.*?)</username>',page)\n",
    "    category=re.findall(patt_category,page)\n",
    "    \n",
    "    txt=re.findall(r'<text.*?>(.*?)</text>',page)\n",
    "    if title:\n",
    "        title=title[0]\n",
    "        if any_in(title,pages_todrop):\n",
    "            return '',[],[],[]\n",
    "    else:\n",
    "        return '',[],[],[]\n",
    "    if txt:\n",
    "        txt=txt[0]\n",
    "    else:\n",
    "        return '',[],[],[]\n",
    "    \n",
    "    pos=[]\n",
    "    for pat in terminal_sections:\n",
    "        pos+=[item.span()[0] for item in re.finditer(r'={2,3} {0,1}%s {0,1}={2,3}'%pat ,txt)]\n",
    "                \n",
    "    pos.sort()\n",
    "    pos=pos[0] if len(pos) else len(txt)\n",
    "    txt=txt[:pos]\n",
    "    \n",
    "    #remove latex ecuations\n",
    "    txt=re.sub(r'[<|&lt;]math.*?/math[&gt;|>]',' ',txt)\n",
    "    \n",
    "    #Remove Boxes\n",
    "    txt=re.sub(r'\\{\\{Start box\\}\\}.*?\\{\\{End box\\}\\}',' ',txt)\n",
    "    \n",
    "    #remove html divisions\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('div'),' ',txt)\n",
    "    \n",
    "    #remove graphs\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('graph'),' ',txt)\n",
    "    \n",
    "    #remove galeries\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('gallery'),' ',txt)\n",
    "\n",
    "    \n",
    "    \n",
    "    #remove citations\n",
    "    if '{{' in txt:\n",
    "        \n",
    "        for l in  get_links(txt,r'\\{\\{',r'\\}\\}'):\n",
    "            rpl=''\n",
    "            cita= re.findall(patt_citation,l)\n",
    "            if cita:\n",
    "                rpl=cita[0]\n",
    "            txt=txt.replace(l,rpl)\n",
    "\n",
    "    #remove tables\n",
    "    txt=txt.replace('&lt;table','&lt; {|').replace( '/table&gt;','|} &gt;')\n",
    "    txt=txt.replace('&lt;TABLE','&lt; {|').replace( '/TABLE&gt;','|} &gt;')\n",
    "    \n",
    "    if '{|' in txt:\n",
    " \n",
    "        for l  in get_links(txt,r'\\{\\|',r'\\|\\}'):\n",
    "            rpl=''\n",
    "            txt=txt.replace(l,rpl)\n",
    " \n",
    "    #remove links\n",
    "    if '[[' in txt:\n",
    "        for l in get_links(txt,r'\\[\\[',r'\\]\\]'):\n",
    "\n",
    "            rpl=''\n",
    "            if not any_in(l,list(':/')):\n",
    "                m=l.strip('[]')\n",
    "                rpl=m.split('|')[1]  if '|' in l else m\n",
    "\n",
    "            txt=txt.replace(l,rpl)\n",
    "        \n",
    "   \n",
    "    #remove special cases of references\n",
    "\n",
    "    txt=re.sub(r'&lt; ?Ref.*?/ref?&gt;',' ',txt)\n",
    "   \n",
    "    #remove sections\n",
    "    \n",
    "    for tag in ['div','ref','nowiki','graph','timeline','center','Center','syntaxhighlight','sub','sup','span','time','small','big','gallery','imagemap']:\n",
    "        txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format(tag),' ',txt)\n",
    "        txt=re.sub(r'&lt; ?{}.*?/&gt;'.format(tag),' ',txt)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    txt=re.sub(r'&lt;noinclude&gt;',' ',txt)\n",
    "    txt=re.sub(r'{{nowrap.*?}}',' ',txt) \n",
    "    \n",
    "    txt=re.sub(r'\\[http.*?\\]','',txt)\n",
    "    \n",
    "    txt=re.sub(r'\\{.*?\\|left','',txt)\n",
    "    txt=re.sub(r'/{0,9}center {0,20}\\|{0,9}','',txt)\n",
    "    txt=re.sub(r'nbsp;',' ',txt)\n",
    "    txt=re.sub(r'br ?/',' ',txt)\n",
    "    \n",
    "    \n",
    "    txt=re.sub(r'\\|{1,20}left','',txt)\n",
    "    txt=re.sub(r'/{1,20}math','',txt)\n",
    "    txt=re.sub(r'\\\\frac','',txt)\n",
    "   \n",
    "    \n",
    "    #rid off misswrited numbers\n",
    "\n",
    "    for p in re.findall(r'([0-9]+\\. {0,9}[0-9]+)',txt):\n",
    "\n",
    "        q=re.sub('\\. {0,9}','',p)\n",
    "        txt=re.sub(p,q,txt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Ad hoc\n",
    "    txt=re.sub(r'\\|wid.*?top\\|','',txt);\n",
    "    txt=re.sub(r'\\|\\|.*?;\\|','',txt);\n",
    "    txt=re.sub(r'\\|?rowspan.*?\\|[(Clas)|(Des)].*?[á|\\|]','',txt);\n",
    "    txt=re.sub(r'\\| ?colspan.*?[\\.|\\|]','',txt);\n",
    "    txt=re.sub(r'colspan=.*?[!|\\|]','',txt);\n",
    "    txt=re.sub(r'rowspan=.*?[!|\\|]','',txt);\n",
    "    txt=re.sub(r'\\|[\\-| ]bg.*?\\.','',txt);\n",
    "    txt=re.sub(r'\\|vh?align.*?\\|','',txt);\n",
    "    txt=re.sub(r'\\|? ?style=.*?[\\.|\\|]','',txt);\n",
    "    txt=re.sub(r'| ?align=|','',txt)\n",
    "    txt=re.sub(r'{| ?class=wikitable.*?\\|-','',txt)\n",
    "    txt=re.sub(r'{| ?class=wikitable.*?!','',txt)\n",
    "    txt=re.sub(r'\\| vtop \\| {1,2}\\| width=50% vtop \\|','',txt)\n",
    "    \n",
    "    #Tidy text for sent tokenize\n",
    "    txt=re.sub(r'etc\\.','etc…',txt)\n",
    "    txt=re.sub(r' ?={2,20} ?','. ',txt)\n",
    "    txt=re.sub(r'\\([. ]*?\\)','',txt)\n",
    "    txt=re.sub(r'\\. *?\\.','. ',txt)\n",
    "    txt=re.sub(r'\\.{2,20}','. ',txt)\n",
    "    txt=txt.replace('}',' ')\n",
    "    txt=re.sub(r'\\'{2,20}','\\'',txt)\n",
    "    txt=re.sub(r'&.*?;','',txt)\n",
    "     \n",
    "    \n",
    "    txt=[item.strip() for item in sent_tok(txt)]\n",
    "    \n",
    "    return title, category, contributors, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "021a90a0-50a4-4136-9f8e-5322d7442c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet='A-ZÁÂÉÊÍÎÓÔÚÛÜÑÇ'\n",
    "alpha_text=lambda x: re.sub('[^{} -]'.format(alphabet+alphabet.lower()),'',x)\n",
    "transpose=lambda x: list(zip(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe30afc-0649-4266-9d40-eb7b029cd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    alphabetized=lambda x: re.sub('[^{}]'.format(alphabet+alphabet.lower()),'',x)\n",
    "    \n",
    "    if isinstance(sent,str):\n",
    "        sent=sent.split()\n",
    "    \n",
    "    sent_f=[alphabetized(i) for i in sent if alphabetized(i)]\n",
    "    if not(sent_f):\n",
    "        return None\n",
    "    sent_f=[sent_f[0] if len(sent_f[0])<2 or sent[0].istitle() else '']+[i for i in sent_f[1:] if  i.islower()] \n",
    "    sent_f=[i.lower() for i in sent_f if i]\n",
    "    return sent_f\n",
    "    \n",
    "\n",
    "def basic_feat(text):\n",
    "    \n",
    "    nsent=0\n",
    "    ntok=[]\n",
    "    nword=[]\n",
    "    sentences=[]\n",
    "    stream=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for item in text:\n",
    "        sent=[s for s in item.split() if len(s)<MAX_CHAR_TOKEN]\n",
    "        \n",
    "        if len(sent)<MIN_TOKENS :\n",
    "            continue\n",
    "        \n",
    "        sent_f=clean_text(sent)\n",
    "        if sent_f:\n",
    "            nsent+=1\n",
    "            ntok.append(len(sent))\n",
    "            nword.append(len(sent_f))\n",
    "                                                        \n",
    "            stream+=sent_f\n",
    "            sentences.append(' '.join(sent))\n",
    "        \n",
    "    return nsent,ntok,nword,Counter(stream),sentences         \n",
    "\n",
    "# \"valor\" de una frase suma de los tf-idf de los tokens de la frase (promedio por token)\n",
    "#value=lambda x: sum([tfidf[key]/len(x.split()) for key in x.split() if key in tfidf.keys()])\n",
    "\n",
    "def basic_stats(vals):\n",
    "    vals=np.array(vals)\n",
    "    if vals.size >0:\n",
    "        \n",
    "        Sh=np.array(list(Counter(vals).values()))\n",
    "        Sh=Sh/Sh.sum()\n",
    "        res=[vals.mean(),vals.std(),vals.min(),vals.max(),-(Sh*np.log(Sh)).sum()]\n",
    "        res+=list(np.quantile(vals,[0.25,0.5,0.75]))\n",
    "        return {key:val for key,val in zip(['mean','std','min','max','Sh','Q25','Q50','Q75'],\n",
    "                                           res)}\n",
    "    \n",
    "    \n",
    "    return {key:val for key,val in zip(['mean','std','min','max','Sh','Q25','Q50','Q75'],\n",
    "                                           [0]*8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b57df5-50eb-431e-a549-15822d6f109e",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "* `MIN_TOKENS`: minimum number of tokens for a valid sentence\n",
    "* `MIN_SENTS`: minumum number of valid sentences for a valid document\n",
    "* `MAX_CHAR_TOKEN`: maximum number of characters in a valid token\n",
    "* `MIN_DOCS`: minimum number of document frequency to be included in tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "251e617c-c3f5-4bfb-8c09-a0639d184b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path.cwd()\n",
    "MIN_TOKENS=4\n",
    "MIN_SENTS=2\n",
    "MAX_CHAR_TOKEN=50\n",
    "MIN_DOCS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c0b944-98ef-4e13-bc81-a2655f5ced5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 9.64 s, total: 39.2 s\n",
      "Wall time: 56.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cpgl=' '.join(list(path.glob('*.xml'))[0].read_text(encoding='UTF8').split())\n",
    "\n",
    "pages=(re.findall('<page>(.*?)</page>',cpgl))\n",
    "\n",
    "with open('pages_wiki.pkl','wb') as fich:\n",
    "    pickle.dump(pages,fich)\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1bb9b90-b5c0-4c42-91bd-d9202d658a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pages_wiki.pkl','rb') as fich:\n",
    "    pages=pickle.load(fich)\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc94598-9744-4a73-9fd2-5be810b76bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(pages):\n",
    "    docs=[]\n",
    "    pgs=[]\n",
    "    for indx,page in enumerate(pages):\n",
    "        try:\n",
    "            title,category,user,text=clean_page(page)\n",
    "            #Filter internal pages\n",
    "            if not title or not text:\n",
    "                continue\n",
    "           \n",
    "            \n",
    "            #Filter documentns by length\n",
    "            test=[alpha_text(item).split() for item in text]\n",
    "            \n",
    "            test=[item for item in test if len(item)>MIN_TOKENS]\n",
    "\n",
    "            if len(test)>MIN_SENTS:\n",
    "                docs.append((title,category,user,text))\n",
    "                pgs.append(page)\n",
    "        except:\n",
    "            print('Failure: ',page)\n",
    "           \n",
    "    return docs,pgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405beac8-c9a6-4f40-a6fa-d70d241a9804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 2.58 s, total: 9.41 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles=[]\n",
    "selected_pages=[]\n",
    "for batch in vectorice(iterable_list=pages,  thread_function=thread_function, max_workers=1000):\n",
    "    d,p=(batch)\n",
    "    articles+=d\n",
    "    selected_pages+=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f10fe60-28de-4032-ac64-4ab851bc7042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150357"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles20221120_gl.pkl','wb') as fich:\n",
    "    pickle.dump(articles,fich)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8471c526-f306-4311-96b9-05c4321e460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150357"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('selected_pages.pkl','wb') as fich:\n",
    "    pickle.dump(selected_pages,fich)\n",
    "len(selected_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae242e9e-483a-4e7c-974d-1defa2cbd7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150357"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles20221120_gl.pkl','rb') as fich:\n",
    "    articles=pickle.load(fich)\n",
    "articles=[document(*item) for item in articles]\n",
    "len(articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6d84d-f3d9-424d-847d-d638ca380f10",
   "metadata": {},
   "source": [
    "# Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "775e5b60-1d0c-4dde-8e1b-b677e1122d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 1206\n",
      "Most active users:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('InternetArchiveBot', 26692),\n",
       " ('Breogan2008', 23136),\n",
       " ('BanjoBot 2.0', 11988),\n",
       " ('Breobot', 9271),\n",
       " ('Corribot', 8926),\n",
       " ('HombreDHojalata', 6857),\n",
       " ('Chairego apc', 6075),\n",
       " ('Estevoaei', 6036),\n",
       " ('Zaosbot', 5430),\n",
       " ('Xanetas', 2847),\n",
       " ('Xas', 2626),\n",
       " ('Alfonso Márquez', 1963),\n",
       " ('MAGHOI', 1850),\n",
       " ('Miguelferig', 1728),\n",
       " ('RubenWGA', 1640),\n",
       " ('Beninho', 1497),\n",
       " ('Vitoriaogando', 1324),\n",
       " ('Chairebot', 1221),\n",
       " ('Moedagalega', 1148),\n",
       " ('Elisardojm', 1087)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users=unravel([item.user for item in articles])\n",
    "users=Counter(users).most_common()\n",
    "print(f'Total number of users: {len(users)}')\n",
    "print('Most active users:')\n",
    "users[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde10902-0641-44cb-8be7-4b4393fad3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "If can be assumed that a user name that contains 'bot' in it identifies a bot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32df2aa4-521a-4bbd-9b69-e6d821bc22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bots: 20\n",
      "Total articles: 64634,  42.987%\n",
      "number of articles by bot\n",
      "\tmean: 3231.7\n",
      "\tmin: 1\n",
      "\tQ25: 5.75\n",
      "\tQ50: 21.5\n",
      "\tQ75: 2273.25\n",
      "\tmax: 26692\n",
      "\tSh: 2.6923109941417858\n",
      "\n",
      "\n",
      "\n",
      "User name       \tnº articles \t%articles\n",
      "InternetArchiveBot   \t26692 \t\t17.752\n",
      "BanjoBot 2.0         \t11988 \t\t7.973\n",
      "Breobot              \t9271 \t\t6.166\n",
      "Corribot             \t8926 \t\t5.937\n",
      "Zaosbot              \t5430 \t\t3.611\n",
      "Chairebot            \t1221 \t\t0.812\n",
      "Aosbot               \t818 \t\t0.544\n",
      "Addbot               \t131 \t\t0.087\n",
      "BotDHojalata         \t70 \t\t0.047\n",
      "EmausBot             \t32 \t\t0.021\n",
      "Escarbot             \t11 \t\t0.007\n",
      "Xqbot                \t11 \t\t0.007\n",
      "KLBot2               \t10 \t\t0.007\n",
      "Texvc2LaTeXBot       \t9 \t\t0.006\n",
      "BanjoBot             \t7 \t\t0.005\n",
      "MGA73bot             \t2 \t\t0.001\n",
      "Prebot               \t2 \t\t0.001\n",
      "TohaomgBot           \t1 \t\t0.001\n",
      "Hector Bottai        \t1 \t\t0.001\n",
      "Jembot               \t1 \t\t0.001\n"
     ]
    }
   ],
   "source": [
    "bots={key:val for key,val in users if 'bot' in key or 'Bot' in key}\n",
    "print(f'Total number of bots: {len(bots)}')\n",
    "print(f'Total articles: {sum(bots.values())},  {100*sum(bots.values())/len(articles):0.3f}%')\n",
    "sts=basic_stats(list(bots.values()))\n",
    "print('number of articles by bot')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "    \n",
    "print('\\n\\n')\n",
    "print('User name       \\tnº articles \\t%articles')\n",
    "for key,val in Counter(bots).most_common():\n",
    "    while len(key)<20:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbbc772-d2c0-4419-9cb8-f1f9a8aa97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of human contributors: 1186\n",
      "number of articles by human\n",
      "\tmean: 70.94940978077571\n",
      "\tmin: 1\n",
      "\tQ25: 1.0\n",
      "\tQ50: 1.0\n",
      "\tQ75: 3.0\n",
      "\tmax: 23136\n",
      "\tSh: 1.9873030119156645\n",
      "\n",
      "\n",
      "\n",
      "Users which accounts for 90.0 % of human articles\n",
      "User name                 \tnº articles \t%articles\n",
      "Breogan2008               \t23136 \t\t15.387\n",
      "HombreDHojalata           \t6857 \t\t4.56\n",
      "Chairego apc              \t6075 \t\t4.04\n",
      "Estevoaei                 \t6036 \t\t4.014\n",
      "Xanetas                   \t2847 \t\t1.893\n",
      "Xas                       \t2626 \t\t1.747\n",
      "Alfonso Márquez           \t1963 \t\t1.306\n",
      "MAGHOI                    \t1850 \t\t1.23\n",
      "Miguelferig               \t1728 \t\t1.149\n",
      "RubenWGA                  \t1640 \t\t1.091\n",
      "Beninho                   \t1497 \t\t0.996\n",
      "Vitoriaogando             \t1324 \t\t0.881\n",
      "Moedagalega               \t1148 \t\t0.764\n",
      "Elisardojm                \t1087 \t\t0.723\n",
      "Jglamela                  \t1041 \t\t0.692\n",
      "Xosema                    \t982 \t\t0.653\n",
      "HacheDous=0               \t962 \t\t0.64\n",
      "Maria zaos                \t917 \t\t0.61\n",
      "CommonsDelinker           \t865 \t\t0.575\n",
      "Norrin strange            \t862 \t\t0.573\n",
      "One2                      \t818 \t\t0.544\n",
      "Lameiro                   \t762 \t\t0.507\n",
      "Ourol                     \t754 \t\t0.501\n",
      "Servando2                 \t732 \t\t0.487\n",
      "Gallaecio                 \t680 \t\t0.452\n",
      "Banjo                     \t649 \t\t0.432\n",
      "Tfeliz                    \t581 \t\t0.386\n",
      "Adorian                   \t567 \t\t0.377\n",
      "Xoio                      \t513 \t\t0.341\n",
      "Gasparoff                 \t504 \t\t0.335\n",
      "Lles                      \t442 \t\t0.294\n",
      "FleaRHCP                  \t382 \t\t0.254\n",
      "Ogaiago                   \t365 \t\t0.243\n",
      "Cossue                    \t363 \t\t0.241\n",
      "Alhadis                   \t362 \t\t0.241\n",
      "Mark Gasoline             \t350 \t\t0.233\n",
      "Fendetestas               \t316 \t\t0.21\n",
      "Calq                      \t313 \t\t0.208\n",
      "Ogalego.gal               \t301 \t\t0.2\n",
      "Krisko                    \t296 \t\t0.197\n",
      "Piquito                   \t292 \t\t0.194\n"
     ]
    }
   ],
   "source": [
    "human={key:val for key,val in users if not ('bot' in key or 'Bot' in key)}\n",
    "print(f'Total number of human contributors: {len(human)}')\n",
    "sts=basic_stats(list(human.values()))\n",
    "print('number of articles by human')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "\n",
    "print('\\n\\n')\n",
    "limit=0.90\n",
    "print(f'Users which accounts for {100*limit} % of human articles')\n",
    "print('User name                 \\tnº articles \\t%articles')\n",
    "acum=0\n",
    "total=sum(list(human.values()))\n",
    "for key,val in Counter(human).most_common():\n",
    "    if acum>limit:\n",
    "        break\n",
    "    acum+=val/total\n",
    "    while len(key)<25:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff859b-e771-40ea-8eaf-6ad7eb3ff037",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ae2564-0139-464f-9cc6-05ad8fa9eee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of categories used: 69879\n",
      "number of articles by category\n",
      "\tmean: 6.983643154595801\n",
      "\tmin: 1\n",
      "\tQ25: 1.0\n",
      "\tQ50: 1.0\n",
      "\tQ75: 4.0\n",
      "\tmax: 5065\n",
      "\tSh: 2.1221845851497263\n",
      "\n",
      "\n",
      "\n",
      "Category                  \t\t\t\t\t\tnº articles \t%articles\n",
      "Personalidades de Galicia sen imaxes                                   \t5065 \t\t3.369\n",
      "Filmes en lingua inglesa                                               \t3203 \t\t2.13\n",
      "Filmes dos Estados Unidos de América                                   \t2730 \t\t1.816\n",
      "Topónimos galegos con etimoloxía                                       \t1595 \t\t1.061\n",
      "Escritores de Galicia en lingua galega                                 \t1427 \t\t0.949\n",
      "Alumnos da Universidade de Santiago de Compostela                      \t1371 \t\t0.912\n",
      "Nados en ano descoñecido                                               \t1366 \t\t0.909\n",
      "Personalidades sen imaxes                                              \t1085 \t\t0.722\n",
      "Escritores de Galicia en lingua castelá                                \t1074 \t\t0.714\n",
      "Personalidades de Galicia sen imaxes finados hai máis de 80 anos       \t994 \t\t0.661\n",
      "Xogadores de baloncesto da NBA                                         \t970 \t\t0.645\n",
      "Nados na Coruña                                                        \t968 \t\t0.644\n",
      "Alcaldes de Galicia durante a democracia                               \t855 \t\t0.569\n",
      "Xogadores de baloncesto dos Estados Unidos de América                  \t814 \t\t0.541\n",
      "Nados en Vigo                                                          \t811 \t\t0.539\n",
      "Nados en Madrid                                                        \t799 \t\t0.531\n",
      "Actores de cine dos Estados Unidos de América                          \t766 \t\t0.509\n",
      "Poetas de Galicia                                                      \t750 \t\t0.499\n",
      "Dianteiros de fútbol                                                   \t739 \t\t0.491\n",
      "Escritores en lingua inglesa                                           \t725 \t\t0.482\n",
      "Centrocampistas de fútbol                                              \t721 \t\t0.48\n",
      "Concellos do Xapón                                                     \t701 \t\t0.466\n",
      "Nados en Santiago de Compostela                                        \t650 \t\t0.432\n",
      "Escritores en lingua castelá                                           \t643 \t\t0.428\n",
      "Actores de televisión dos Estados Unidos de América                    \t624 \t\t0.415\n",
      "Asasinados polo bando nacional en Galicia                              \t624 \t\t0.415\n",
      "Satélites artificiais                                                  \t589 \t\t0.392\n",
      "Xogadores do Deportivo da Coruña                                       \t578 \t\t0.384\n",
      "Filmes en lingua castelá                                               \t570 \t\t0.379\n",
      "Defensas de fútbol                                                     \t568 \t\t0.378\n",
      "Finados en 1936                                                        \t568 \t\t0.378\n",
      "Parroquias de Galicia baixo a advocación de santa María                \t536 \t\t0.356\n",
      "Animais descritos en 1758                                              \t521 \t\t0.347\n",
      "Nados en Barcelona                                                     \t490 \t\t0.326\n",
      "Xogadores de baloncesto da ACB                                         \t490 \t\t0.326\n",
      "Filmes do Reino Unido                                                  \t477 \t\t0.317\n",
      "Finados en ano descoñecido                                             \t469 \t\t0.312\n",
      "Filmes de Francia                                                      \t458 \t\t0.305\n",
      "Nados en Pontevedra                                                    \t456 \t\t0.303\n",
      "Taxons descritos por Linné                                             \t449 \t\t0.299\n",
      "Alcaldes de Galicia durante o franquismo e a transición                \t444 \t\t0.295\n",
      "Nados en Ourense                                                       \t428 \t\t0.285\n",
      "Alcaldes de Galicia durante a Restauración                             \t428 \t\t0.285\n",
      "Alcaldes de Galicia polo PPdeG                                         \t413 \t\t0.275\n",
      "Xogadores do Celta de Vigo                                             \t407 \t\t0.271\n",
      "Exilio galego                                                          \t405 \t\t0.269\n",
      "Filmes de Warner Bros.                                                 \t396 \t\t0.263\n",
      "Represaliados polo franquismo                                          \t396 \t\t0.263\n",
      "Nados en Ferrol                                                        \t394 \t\t0.262\n",
      "Nados en París                                                         \t389 \t\t0.259\n",
      "Artigos creados co asistente para a creación de artigos                \t386 \t\t0.257\n",
      "Escritores dos Estados Unidos de América                               \t383 \t\t0.255\n",
      "Lugares de Galicia despoboados                                         \t370 \t\t0.246\n",
      "Nados en 1964                                                          \t367 \t\t0.244\n",
      "Deputados de Galicia no Congreso dos Deputados                         \t365 \t\t0.243\n",
      "Escritores de Galicia coa obra no dominio público                      \t362 \t\t0.241\n",
      "Nados en 1948                                                          \t358 \t\t0.238\n",
      "Xornalistas de Galicia                                                 \t358 \t\t0.238\n",
      "Nados en Nova York                                                     \t357 \t\t0.237\n",
      "Filmes de España                                                       \t352 \t\t0.234\n",
      "Filmes ambientados en Nova York                                        \t350 \t\t0.233\n",
      "Filmes en lingua francesa                                              \t347 \t\t0.231\n",
      "Membros correspondentes da Real Academia Galega                        \t347 \t\t0.231\n",
      "Nados en Londres                                                       \t346 \t\t0.23\n",
      "Astronautas dos Estados Unidos de América                              \t343 \t\t0.228\n",
      "Nados en 1961                                                          \t342 \t\t0.227\n",
      "Nados en 1952                                                          \t342 \t\t0.227\n",
      "Escoltas de baloncesto                                                 \t342 \t\t0.227\n",
      "Ala-pivotes de baloncesto                                              \t341 \t\t0.227\n",
      "Paseo da Fama de Hollywood                                             \t339 \t\t0.225\n",
      "Nados en 1965                                                          \t339 \t\t0.225\n",
      "Mestres de Galicia                                                     \t332 \t\t0.221\n",
      "Nados en 1956                                                          \t332 \t\t0.221\n",
      "Nados en 1959                                                          \t331 \t\t0.22\n",
      "Nados en 1962                                                          \t329 \t\t0.219\n",
      "Escritores en lingua francesa                                          \t329 \t\t0.219\n",
      "Nados en 1967                                                          \t327 \t\t0.217\n",
      "Estadounidenses de ascendencia inglesa                                 \t327 \t\t0.217\n",
      "Nados en Lugo                                                          \t327 \t\t0.217\n",
      "Nados en 1946                                                          \t327 \t\t0.217\n",
      "Plantas medicinais                                                     \t327 \t\t0.217\n",
      "Alas de baloncesto                                                     \t325 \t\t0.216\n",
      "Nados en 1958                                                          \t324 \t\t0.215\n",
      "Bases de baloncesto                                                    \t321 \t\t0.213\n",
      "Lugares de Palas de Rei                                                \t321 \t\t0.213\n",
      "Pivotes de baloncesto                                                  \t320 \t\t0.213\n",
      "Secuelas de filmes dos Estados Unidos de América                       \t319 \t\t0.212\n",
      "Alcaldes de Galicia durante a II República                             \t318 \t\t0.211\n",
      "Nados en 1966                                                          \t317 \t\t0.211\n",
      "Nados en 1949                                                          \t315 \t\t0.21\n",
      "Nados en 1947                                                          \t314 \t\t0.209\n",
      "Wikipedia:Páxinas con traducións non revisadas                         \t314 \t\t0.209\n",
      "Nados en 1951                                                          \t313 \t\t0.208\n",
      "Nados en 1954                                                          \t312 \t\t0.208\n",
      "Galegos da Arxentina                                                   \t310 \t\t0.206\n",
      "Nados en 1969                                                          \t308 \t\t0.205\n",
      "Nados en 1957                                                          \t308 \t\t0.205\n",
      "Nados en 1955                                                          \t306 \t\t0.204\n",
      "Nados en 1960                                                          \t306 \t\t0.204\n",
      "Nados en 1944                                                          \t305 \t\t0.203\n",
      "Nados en 1963                                                          \t305 \t\t0.203\n",
      "Grupos musicais de rock alternativo                                    \t304 \t\t0.202\n",
      "Nados en 1977                                                          \t304 \t\t0.202\n",
      "Nados en 1942                                                          \t303 \t\t0.202\n",
      "Nados en 1943                                                          \t301 \t\t0.2\n",
      "Artigos que toda Wikipedia debería ter (Ciencia)                       \t300 \t\t0.2\n",
      "Nados en 1968                                                          \t299 \t\t0.199\n",
      "Nados en 1971                                                          \t298 \t\t0.198\n",
      "Escritores en lingua galega                                            \t297 \t\t0.198\n",
      "Nados en 1970                                                          \t297 \t\t0.198\n",
      "Física                                                                 \t296 \t\t0.197\n",
      "Personalidades da empresa de Galicia                                   \t294 \t\t0.196\n",
      "Series de televisión dos Estados Unidos de América                     \t292 \t\t0.194\n",
      "Proteínas                                                              \t292 \t\t0.194\n",
      "Nados en 1950                                                          \t290 \t\t0.193\n",
      "Militares de Galicia                                                   \t290 \t\t0.193\n",
      "Nados en 1976                                                          \t289 \t\t0.192\n",
      "Escritores de Galicia                                                  \t289 \t\t0.192\n",
      "Nados en 1975                                                          \t289 \t\t0.192\n",
      "Pilotos das 24 Horas de Le Mans                                        \t287 \t\t0.191\n",
      "Eleccións municipais en Galicia                                        \t284 \t\t0.189\n",
      "Nados en 1953                                                          \t283 \t\t0.188\n",
      "Alumnos da Universidade Complutense de Madrid                          \t280 \t\t0.186\n",
      "Nados en 1973                                                          \t279 \t\t0.186\n",
      "Nados en 1974                                                          \t277 \t\t0.184\n",
      "Nados en 1981                                                          \t275 \t\t0.183\n",
      "Nados en 1979                                                          \t275 \t\t0.183\n",
      "Nados en 1945                                                          \t275 \t\t0.183\n",
      "Políticos do PSOE                                                      \t275 \t\t0.183\n",
      "Filmes de Universal Pictures                                           \t270 \t\t0.18\n",
      "Guerrilleiros antifranquistas de Galicia                               \t268 \t\t0.178\n",
      "Nados en 1980                                                          \t267 \t\t0.178\n",
      "Nados en 1978                                                          \t264 \t\t0.176\n",
      "Nados en 1972                                                          \t263 \t\t0.175\n",
      "Nados en 1940                                                          \t258 \t\t0.172\n",
      "Directores de cine dos Estados Unidos de América                       \t258 \t\t0.172\n",
      "Illas do Pacífico                                                      \t255 \t\t0.17\n",
      "Actores de teatro dos Estados Unidos de América                        \t254 \t\t0.169\n",
      "Alcaldes de Galicia polo PSdeG-PSOE                                    \t254 \t\t0.169\n",
      "Filmes de Columbia Pictures                                            \t254 \t\t0.169\n",
      "Nados en 1986                                                          \t252 \t\t0.168\n",
      "Lugares de Castro de Rei                                               \t252 \t\t0.168\n",
      "Finados en 1937                                                        \t250 \t\t0.166\n",
      "Lugares das Pontes de García Rodríguez                                 \t248 \t\t0.165\n",
      "Nados en 1932                                                          \t246 \t\t0.164\n",
      "Nados en 1988                                                          \t246 \t\t0.164\n",
      "Santos do catolicismo                                                  \t245 \t\t0.163\n",
      "Nados en 1928                                                          \t245 \t\t0.163\n",
      "Futbolistas gañadores da Liga de Campións da UEFA                      \t245 \t\t0.163\n",
      "Nados en 1936                                                          \t245 \t\t0.163\n",
      "Papas                                                                  \t245 \t\t0.163\n",
      "Nados en 1984                                                          \t244 \t\t0.162\n",
      "Nados en 1985                                                          \t244 \t\t0.162\n",
      "Xogadores da selección de fútbol de España                             \t241 \t\t0.16\n",
      "Nados en 1941                                                          \t241 \t\t0.16\n",
      "Nados en 1930                                                          \t240 \t\t0.16\n",
      "Filmes distribuídos en sistemas IMAX                                   \t240 \t\t0.16\n",
      "Personalidades da zooloxía                                             \t240 \t\t0.16\n",
      "Nados en 1987                                                          \t236 \t\t0.157\n",
      "Nados en 1990                                                          \t236 \t\t0.157\n",
      "Personalidades da política de Galicia                                  \t235 \t\t0.156\n",
      "Finados en 2020                                                        \t234 \t\t0.156\n",
      "Nados en 1937                                                          \t232 \t\t0.154\n",
      "Filmes de 20th Century Fox                                             \t231 \t\t0.154\n",
      "Filmes rodados nos Ánxeles                                             \t231 \t\t0.154\n",
      "Nados en 1989                                                          \t231 \t\t0.154\n",
      "Finados en 2021                                                        \t230 \t\t0.153\n",
      "Grupos musicais de indie rock                                          \t230 \t\t0.153\n",
      "Ministros de España                                                    \t229 \t\t0.152\n",
      "Nados en 1982                                                          \t229 \t\t0.152\n",
      "Parroquias de Galicia baixo a advocación de san Pedro                  \t226 \t\t0.15\n",
      "Profesores da Universidade de Santiago de Compostela                   \t226 \t\t0.15\n",
      "Plantas descritas en 1753                                              \t223 \t\t0.148\n",
      "Nados en 1934                                                          \t222 \t\t0.148\n",
      "Lugares de Carballedo                                                  \t221 \t\t0.147\n",
      "Galegos de Cuba                                                        \t220 \t\t0.146\n",
      "Termos botánicos                                                       \t219 \t\t0.146\n",
      "Termos zoolóxicos                                                      \t219 \t\t0.146\n",
      "Escritores en lingua portuguesa                                        \t219 \t\t0.146\n",
      "Filmes de Alemaña                                                      \t219 \t\t0.146\n",
      "Nados en 1935                                                          \t218 \t\t0.145\n",
      "Nados en 1939                                                          \t218 \t\t0.145\n",
      "Nados en 1931                                                          \t217 \t\t0.144\n",
      "Personalidades da música de Galicia                                    \t216 \t\t0.144\n",
      "Gardametas de fútbol                                                   \t216 \t\t0.144\n",
      "Nados en 1933                                                          \t216 \t\t0.144\n",
      "Filmes preservados no National Film Registry                           \t215 \t\t0.143\n",
      "Filmes de Paramount Pictures                                           \t215 \t\t0.143\n",
      "Profesores de educación secundaria                                     \t214 \t\t0.142\n",
      "Premios Nobel dos Estados Unidos de América                            \t212 \t\t0.141\n",
      "Escritores de Francia                                                  \t212 \t\t0.141\n",
      "Lugares de Ortigueira                                                  \t212 \t\t0.141\n",
      "Artigos de Galicia sen imaxes                                          \t211 \t\t0.14\n",
      "Nados en 1991                                                          \t211 \t\t0.14\n",
      "Parroquias de Galicia baixo a advocación de Santiago o Maior           \t209 \t\t0.139\n",
      "Nados en Buenos Aires                                                  \t208 \t\t0.138\n",
      "Finados en 2012                                                        \t207 \t\t0.138\n",
      "Moedas fóra de curso                                                   \t207 \t\t0.138\n",
      "Artigos destacados                                                     \t206 \t\t0.137\n",
      "Membros da Royal Society                                               \t206 \t\t0.137\n",
      "Nados en 1926                                                          \t206 \t\t0.137\n",
      "Finados en 2018                                                        \t205 \t\t0.136\n",
      "Xogadores do Club Baloncesto Breogán                                   \t205 \t\t0.136\n",
      "Artigos que toda Wikipedia debería ter (Biografías)                    \t204 \t\t0.136\n",
      "Nados en 1983                                                          \t204 \t\t0.136\n",
      "Actores de voz dos Estados Unidos de América                           \t202 \t\t0.134\n",
      "Nados en 1921                                                          \t202 \t\t0.134\n",
      "Empresas de Galicia                                                    \t201 \t\t0.134\n",
      "Nados en 1908                                                          \t200 \t\t0.133\n"
     ]
    }
   ],
   "source": [
    "categories=unravel([item.category for item in articles])\n",
    "categories=Counter(categories).most_common()\n",
    "print(f'Total number of categories used: {len(categories)}')\n",
    "sts=basic_stats(list(dict(categories).values()))\n",
    "print('number of articles by category')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('Category                  \\t\\t\\t\\t\\t\\tnº articles \\t%articles')\n",
    "for key,val in categories:\n",
    "    if val<200:\n",
    "        break\n",
    "    while len(key)<70:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e8573-c6f5-4720-bc20-59efa16f3c6a",
   "metadata": {},
   "source": [
    "# Basic features  \n",
    "Language-agnostic preliminary analysis, so there is not misspelled words control or token lemmatization.  \n",
    "It relies on three routines:  \n",
    "* `clean_text`: Used to create the _Bag of Words_ (bow) for each article. Only alphabetical chars are allowed. The input is a sentence (list of tokens or string) and the output is a list of alphabetical tokens.\n",
    "* `basic_feat`: the main routine. The input is the extracted text of each article. This is processed to get:\n",
    "    * The number of sentences in the article, an `int`\n",
    "    * A list with the number of tokens in each sentence of the document\n",
    "    * A list with the number of tokens in the output of `clean_text` applied to each sentence; these are called _words_.\n",
    "    * A dictionary with the bow of the document, as defined above.\n",
    "    * A list with the sentences of the article text, after applied the `MAX_CHAR_TOKEN` filter and the `MIN_TOKENS` filter\n",
    "* `basic_stats`: the input is a list of numerical values and returns a dictionary with the mean, standard deviation, maximum, minimun, informational entropy and th quantile values for 25%, 50% (median) and 75%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "807be7e8-3573-4248-b675-2ec7e9b33ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(arts):\n",
    "    bows=[]\n",
    "    feats=[]\n",
    "    sents=[]\n",
    "\n",
    "    for art in arts:\n",
    "        nsent,ntok,nword,bow,sentences=basic_feat(art.text)\n",
    "        sents.append(sentences)\n",
    "        bows.append(bow)\n",
    "        numt=basic_stats(ntok)\n",
    "        numw=basic_stats(nword)\n",
    "        nums=basic_stats(list(bow.values()))\n",
    "        feats.append([art.title,nsent,numt['mean'],numt['Sh'],numw['mean'],numw['Sh'],nums['Sh'],sum(list(bow.values()))/len(bow) if bow else 0])\n",
    "    return (bows,feats,sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4325a456-3886-4af9-ac18-82e8607fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 s, sys: 2.78 s, total: 14.8 s\n",
      "Wall time: 26.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer=vectorice(articles,thread_function,max_workers=256)\n",
    "\n",
    "bows=[]\n",
    "feats=[]\n",
    "sents=[]\n",
    "\n",
    "for b,f,s in answer:\n",
    "    bows+=(b)\n",
    "    feats+=(f)\n",
    "    sents+=(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0726f-129d-4950-86f9-999e3c7f1c4f",
   "metadata": {},
   "source": [
    "# Basic features\n",
    "Data frame with basic features of each article:\n",
    "* `key`: article title\n",
    "* `nsent`: number of sentences in the article\n",
    "* `mean_tok`: mean of number of tokens per sentence in article\n",
    "* `Sh_tok`: Informational entropy of tokens per sentence in article, in _nats_\n",
    "* `mean_word`: mean of number of alphabetical tokens (`clean_text` output) per sentence in article\n",
    "* `Sh_word`: Informational entropy of alphabetical tokens per sentence in article\n",
    "* `Sh_bow`: Informational entropy of article's Bag of Words (bow) \n",
    "* `IL`: Lexical index, defined as $\\cfrac{\\#(words~in~article)}{\\#(unique~words)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cc98a50-f71a-4d12-951a-55ed6a73bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsent</th>\n",
       "      <th>mean_tok</th>\n",
       "      <th>Sh_tok</th>\n",
       "      <th>mean_word</th>\n",
       "      <th>Sh_word</th>\n",
       "      <th>Sh_bow</th>\n",
       "      <th>IL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.766529</td>\n",
       "      <td>19.159613</td>\n",
       "      <td>1.999847</td>\n",
       "      <td>15.363247</td>\n",
       "      <td>1.952210</td>\n",
       "      <td>0.867762</td>\n",
       "      <td>1.755467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.690028</td>\n",
       "      <td>6.282143</td>\n",
       "      <td>0.706396</td>\n",
       "      <td>5.842517</td>\n",
       "      <td>0.705394</td>\n",
       "      <td>0.239539</td>\n",
       "      <td>0.954842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.194444</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.122449</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.715514</td>\n",
       "      <td>1.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>1.906155</td>\n",
       "      <td>0.868225</td>\n",
       "      <td>1.628788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.875000</td>\n",
       "      <td>2.521772</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.473208</td>\n",
       "      <td>1.016421</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1507.000000</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>4.281662</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>4.145550</td>\n",
       "      <td>2.516510</td>\n",
       "      <td>96.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               nsent       mean_tok         Sh_tok      mean_word  \\\n",
       "count  150357.000000  150357.000000  150357.000000  150357.000000   \n",
       "mean       18.766529      19.159613       1.999847      15.363247   \n",
       "std        33.690028       6.282143       0.706396       5.842517   \n",
       "min         2.000000       4.194444      -0.000000       1.122449   \n",
       "25%         5.000000      14.833333       1.386294      11.250000   \n",
       "50%         9.000000      19.000000       1.945910      15.200000   \n",
       "75%        19.000000      22.875000       2.521772      19.000000   \n",
       "max      1507.000000     101.500000       4.281662      72.500000   \n",
       "\n",
       "             Sh_word         Sh_bow             IL  \n",
       "count  150357.000000  150357.000000  150357.000000  \n",
       "mean        1.952210       0.867762       1.755467  \n",
       "std         0.705394       0.239539       0.954842  \n",
       "min        -0.000000      -0.000000       1.000000  \n",
       "25%         1.386294       0.715514       1.423077  \n",
       "50%         1.906155       0.868225       1.628788  \n",
       "75%         2.473208       1.016421       1.900000  \n",
       "max         4.145550       2.516510      96.608696  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats=pd.DataFrame(feats,columns=[ 'key','nsent','mean_tok','Sh_tok','mean_word','Sh_word','Sh_bow','IL'])\n",
    "feats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a40672e5-0d90-44d4-a152-4fc6474b4737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>nsent</th>\n",
       "      <th>mean_tok</th>\n",
       "      <th>Sh_tok</th>\n",
       "      <th>mean_word</th>\n",
       "      <th>Sh_word</th>\n",
       "      <th>Sh_bow</th>\n",
       "      <th>IL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97980</th>\n",
       "      <td>Cimadevila, Vilaboa, Vilaboa</td>\n",
       "      <td>2</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.206192</td>\n",
       "      <td>1.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96215</th>\n",
       "      <td>A Devesa, Anafreita, Friol</td>\n",
       "      <td>2</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.198515</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96216</th>\n",
       "      <td>O Outeiro, Miraz, Friol</td>\n",
       "      <td>2</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.566086</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96217</th>\n",
       "      <td>As Laxes, Miraz, Friol</td>\n",
       "      <td>2</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.206192</td>\n",
       "      <td>1.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96219</th>\n",
       "      <td>Santo Alberto, San Breixo de Parga, Guitiriz</td>\n",
       "      <td>2</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.566086</td>\n",
       "      <td>1.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14267</th>\n",
       "      <td>Guerra das Malvinas</td>\n",
       "      <td>1098</td>\n",
       "      <td>17.577413</td>\n",
       "      <td>3.587526</td>\n",
       "      <td>14.431694</td>\n",
       "      <td>3.544794</td>\n",
       "      <td>1.506084</td>\n",
       "      <td>4.133020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64619</th>\n",
       "      <td>Historia de Serbia</td>\n",
       "      <td>1102</td>\n",
       "      <td>24.083485</td>\n",
       "      <td>3.861097</td>\n",
       "      <td>20.664247</td>\n",
       "      <td>3.744787</td>\n",
       "      <td>1.551911</td>\n",
       "      <td>4.764017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93579</th>\n",
       "      <td>Eleccións municipais de 2015 en Galicia</td>\n",
       "      <td>1312</td>\n",
       "      <td>8.362043</td>\n",
       "      <td>2.184080</td>\n",
       "      <td>3.057927</td>\n",
       "      <td>1.573469</td>\n",
       "      <td>1.857073</td>\n",
       "      <td>10.872629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106603</th>\n",
       "      <td>Mártires do século XX en España</td>\n",
       "      <td>1346</td>\n",
       "      <td>5.564636</td>\n",
       "      <td>1.622353</td>\n",
       "      <td>1.943536</td>\n",
       "      <td>1.237220</td>\n",
       "      <td>1.608374</td>\n",
       "      <td>4.158983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62167</th>\n",
       "      <td>Historia dos eslavos do sur</td>\n",
       "      <td>1507</td>\n",
       "      <td>25.094891</td>\n",
       "      <td>3.902138</td>\n",
       "      <td>21.678832</td>\n",
       "      <td>3.799351</td>\n",
       "      <td>1.654813</td>\n",
       "      <td>5.349599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150357 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 key  nsent   mean_tok  \\\n",
       "97980                   Cimadevila, Vilaboa, Vilaboa      2  17.000000   \n",
       "96215                     A Devesa, Anafreita, Friol      2  16.000000   \n",
       "96216                        O Outeiro, Miraz, Friol      2  18.000000   \n",
       "96217                         As Laxes, Miraz, Friol      2  16.000000   \n",
       "96219   Santo Alberto, San Breixo de Parga, Guitiriz      2  14.500000   \n",
       "...                                              ...    ...        ...   \n",
       "14267                            Guerra das Malvinas   1098  17.577413   \n",
       "64619                             Historia de Serbia   1102  24.083485   \n",
       "93579        Eleccións municipais de 2015 en Galicia   1312   8.362043   \n",
       "106603               Mártires do século XX en España   1346   5.564636   \n",
       "62167                    Historia dos eslavos do sur   1507  25.094891   \n",
       "\n",
       "          Sh_tok  mean_word   Sh_word    Sh_bow         IL  \n",
       "97980   0.693147  11.000000  0.693147  0.206192   1.157895  \n",
       "96215   0.693147  11.000000  0.693147  0.198515   1.100000  \n",
       "96216   0.693147  15.000000  0.693147  0.566086   1.250000  \n",
       "96217   0.693147  10.500000  0.693147  0.206192   1.105263  \n",
       "96219   0.693147   8.500000  0.693147  0.566086   1.416667  \n",
       "...          ...        ...       ...       ...        ...  \n",
       "14267   3.587526  14.431694  3.544794  1.506084   4.133020  \n",
       "64619   3.861097  20.664247  3.744787  1.551911   4.764017  \n",
       "93579   2.184080   3.057927  1.573469  1.857073  10.872629  \n",
       "106603  1.622353   1.943536  1.237220  1.608374   4.158983  \n",
       "62167   3.902138  21.678832  3.799351  1.654813   5.349599  \n",
       "\n",
       "[150357 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.sort_values('nsent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de8b43-f9a0-4347-b7a5-fa2a18b311ec",
   "metadata": {},
   "source": [
    "# Computing idf\n",
    "\n",
    "[_idf_](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) has a yet long history in information retrieval and gives a way to estimate the relative _value_ of a word, a sentence or a document.\n",
    " The classical definition is $\\mathrm{idf}(t, D) = \\log \\cfrac{N}{|\\{d \\in D: t \\in d\\}|}$; where $N$ is the number of documents, $D$ in the collection (in this context the number of extracted articles), and $|\\{d \\in D: t \\in d\\}|$ the number of documents, $d$, which contains the term $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c88bf-e168-4a2b-975a-a57a6d9f307b",
   "metadata": {},
   "source": [
    "The function `get_tfidf_bows` gets a list with the [_Bag of Wods_](https://en.wikipedia.org/wiki/Bag-of-words_model) for each document as input and returns:\n",
    "* `ndw` the _idf_ for each term in the collection. \n",
    "* `bow` the _Bag of Words_ for the entire collection. A dictionary where the term is the key. The keys in ndw and bow are the same\n",
    "* `tfidf` the _tfidf_ value for each document in the collection: _bow·ndw_\n",
    "\n",
    "All three are python dictionaries where the term is the key. \n",
    "Applying `tfidf` or `ndw` implies that any term not included in these dictionaries has 0 value.\n",
    "\n",
    "Three filters could be applied to construct `nwd` and `tfidf`:\n",
    "* `min_len` : minimum length of the term, so word of 1, 2 or 3 letters have a low probability of be significant.\n",
    "* `min_docs` : minimum number of documents that include the term for the term to be included. It is supposed that this filter removes misspelled words and extremely exotic words.\n",
    "* `todrop` : a set with terms to be excluded arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6b6e728-df79-4ac0-a128-a8a1d18c885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_bows(bows,todrop={},min_len=4,min_docs=MIN_DOCS):\n",
    "    \n",
    "    bow={}\n",
    "    ndw={}\n",
    "    for b in bows:\n",
    "        for key,val in b.items():\n",
    "            if len(key)<min_len or key in todrop:\n",
    "                continue\n",
    "            bow[key]=bow.get(key,0)+val\n",
    "            ndw[key]=ndw.get(key,0)+1\n",
    "        \n",
    "    tfidf={}\n",
    "    ndw={key:val for key,val in ndw.items() if val>min_docs}\n",
    "    total=len(bows)\n",
    "    ndw={key:np.log(total/val) for key,val in ndw.items()}\n",
    "    bow={key:bow[key] for key in ndw.keys()}\n",
    "    total=sum(bow.values())\n",
    "    bow={key:val/total for key,val in bow.items()}\n",
    "    tfidf={key:val*bow[key] for key,val in ndw.items()}\n",
    "    return tfidf,bow,ndw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4946b67-953b-43c2-9c03-d1af167422f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 20.2 ms, total: 6.94 s\n",
      "Wall time: 6.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf,BOW,ndw=get_tfidf_bows(bows,todrop={},min_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00969be4-7cd9-47bb-bb3c-c7d5eaa1dd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('autotróficos', 10.534473384244254),\n",
       " ('sostelos', 10.534473384244254),\n",
       " ('casiodoro', 10.534473384244254),\n",
       " ('aedificatoria', 10.534473384244254),\n",
       " ('alberti', 10.534473384244254),\n",
       " ('reprodutibilidade', 10.534473384244254),\n",
       " ('reinterpretan', 10.534473384244254),\n",
       " ('imitativas', 10.534473384244254),\n",
       " ('trivio', 10.534473384244254),\n",
       " ('poesis', 10.534473384244254),\n",
       " ('valorable', 10.534473384244254),\n",
       " ('aprecialas', 10.534473384244254),\n",
       " ('ordenalas', 10.534473384244254),\n",
       " ('fixalas', 10.534473384244254),\n",
       " ('augatinta', 10.534473384244254)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ndw).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17924964-2462-4a8f-9da8-f7c26cdca2fc",
   "metadata": {},
   "source": [
    "## tfidf sentences based\n",
    "The goal of this work is sentence-based, so it seems plausible compute the idf over an all sentences basis, that is think each sentence as a document for idf calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ff9e7e3-6237-45b6-8ac0-b0ec2cfdd58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2755921"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only sentences with more than 3 tokens\n",
    "sentences=[item for item in unravel(sents) if len(item.split())>3]\n",
    "#only unique sentences\n",
    "sentences=list(set(sentences))\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30d56f9c-4ab8-4b25-8a7d-a6379e95fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 0 ns, total: 2min 20s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_tfidf={}\n",
    "all_bow={}\n",
    "all_ndw={}\n",
    "for s in sentences:\n",
    "    tbow=Counter(clean_text(s))\n",
    "    for key,val in tbow.items():\n",
    "        all_bow[key]=all_bow.get(key,0)+val\n",
    "        all_ndw[key]=all_ndw.get(key,0)+1\n",
    "all_ndw={key:val for key,val in all_ndw.items() if val > MIN_DOCS and len(key)>2}\n",
    "all_bow={key:val for key,val in all_bow.items() if key in all_ndw.keys()}\n",
    "total=sum(list(all_bow.values()))\n",
    "all_bow={key:val/total for key,val in all_bow.items()}\n",
    "total=len(sentences)\n",
    "all_ndw={key:np.log(total/val) for key,val in all_ndw.items()}\n",
    "all_tfidf={key:val*all_bow[key] for key,val in all_ndw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "003ed22d-0ed5-4715-94fd-ed1567f55911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boire', 13.44296788485346),\n",
       " ('prochain', 13.44296788485346),\n",
       " ('pintouna', 13.44296788485346),\n",
       " ('reestruturaron', 13.44296788485346),\n",
       " ('valisoletanos', 13.44296788485346),\n",
       " ('gals', 13.44296788485346),\n",
       " ('calambur', 13.44296788485346),\n",
       " ('adsorbidos', 13.44296788485346),\n",
       " ('retroalimenta', 13.44296788485346),\n",
       " ('mistica', 13.44296788485346),\n",
       " ('amiche', 13.44296788485346),\n",
       " ('laxness', 13.44296788485346),\n",
       " ('esencialemente', 13.44296788485346),\n",
       " ('chaqueña', 13.44296788485346),\n",
       " ('albornoz', 13.44296788485346)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_ndw).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449347a6-4a9e-4ca3-8ee6-65027c4d17bd",
   "metadata": {},
   "source": [
    "## value computation\n",
    "\n",
    "So, there is four plausible computation schemas:\n",
    "* Classical: $tfidf=ndw \\cdot bow$ where $ndw$ is the _idf_ on a document basis and $bow$ the bag of words of the document \n",
    "* Alternative1: $tfidf=ndw \\cdot BOW$ where $BOW$ is the _Bag of Words_ for the collection of documents. These $tfidf$ values are the same for all documents.\n",
    "* Alternative2: the same computation that _alternative1_, but $ndw$ is computed over sentences basis, as explain above\n",
    "* Alternative3: the same computation that _classical_,  but $ndw$ is computed over sentences basis, as explain above\n",
    "\n",
    "Values per sentence are computated with the function `get_value`:\n",
    "* `sent`: the sentence, as list of tokens or string\n",
    "* `tfidf`: the tfidf to apply for computation\n",
    "* `func`: the function to apply to get the sentence value, `sum` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fa417b1-899a-4938-9f02-c71afb5a01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(sent,tfidf,func=np.sum):\n",
    "    if isinstance(sent,str):\n",
    "        sent=sent.split()\n",
    "    res=[tfidf[key] for key in clean_text(sent) if key in tfidf.keys()]\n",
    "    return func(res) if res else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fa664-6cbc-4a5e-a3ad-4f01e4cbd45a",
   "metadata": {},
   "source": [
    "For easy comparison, values results are recorded in a dataframe. Columns are named with a prefix and a suffix. The suffix is related to the method of computation: _Classical_ ==> _class_; _Alternative1_ ==> _alt1_ and so on.\n",
    "The prefix are:\n",
    "* _mean_: mean sentence value of sentences in article\n",
    "* _max_: maximun sentence value in article\n",
    "* _Sh_ : informational entropy of sentences values\n",
    "* _weight_ : summation of sentences values in article weighted by $\\left( 1+\\cfrac{1}{\\#sentences} \\right)$, so the effect of article length is somehow moderated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "454bdda2-8743-4c36-a587-768c170d8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 22.2 ms, total: 2min 49s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Classical\n",
    "v=[]\n",
    "for sent,bow in zip(sents,bows):\n",
    "    total=sum(list(bow.values()))\n",
    "    td={key:val*ndw[key]/total for key,val in bow.items() if key in ndw.keys()}\n",
    "    z=[get_value(s,td) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "\n",
    "values=pd.DataFrame(v,columns=['mean_class','max_class','Sh_class','weight_class'])\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95c898dd-8e8e-4ad6-9de8-51b0bbf55686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 42s, sys: 27.2 ms, total: 2min 42s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative1\n",
    "v=[]\n",
    "for sent in sents:\n",
    "    z=[get_value(s,tfidf) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt1'],values['max_alt1'],values['Sh_alt1'],values['weight_alt1']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f2ba0a-468c-4afc-bd8c-6f2a0baf425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 15.6 ms, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative2\n",
    "v=[]\n",
    "for sent in sents:\n",
    "    z=[get_value(s,all_tfidf) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt2'],values['max_alt2'],values['Sh_alt2'],values['weight_alt2']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a78b1b6-af71-4d81-a56b-e5338c3b6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 11.7 ms, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative3\n",
    "v=[]\n",
    "for sent,bow in zip(sents,bows):\n",
    "    total=sum(list(bow.values()))\n",
    "    td={key:val*all_ndw[key]/total for key,val in bow.items() if key in all_ndw.keys()}\n",
    "    z=[get_value(s,td) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt3'],values['max_alt3'],values['Sh_alt3'],values['weight_alt3']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a2528-ebfa-45b1-ace7-ba23eee850b1",
   "metadata": {},
   "source": [
    "As expected there is high correlation between _Classical_ and _Alternative3_ by one hand and between alternatives _1_ and _2_  for all computed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a10a824-0ac2-4f27-9ed2-634f04c1a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_class</th>\n",
       "      <th>weight_alt1</th>\n",
       "      <th>weight_alt2</th>\n",
       "      <th>weight_alt3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "      <td>150357.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.112770</td>\n",
       "      <td>0.492201</td>\n",
       "      <td>1.210659</td>\n",
       "      <td>8.347714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.759573</td>\n",
       "      <td>0.976010</td>\n",
       "      <td>2.497411</td>\n",
       "      <td>13.152830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.652846</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.748944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.120689</td>\n",
       "      <td>0.119138</td>\n",
       "      <td>0.262486</td>\n",
       "      <td>5.195688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.925040</td>\n",
       "      <td>0.224611</td>\n",
       "      <td>0.534652</td>\n",
       "      <td>6.439734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.319337</td>\n",
       "      <td>0.480366</td>\n",
       "      <td>1.175415</td>\n",
       "      <td>8.684470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1814.410277</td>\n",
       "      <td>50.146877</td>\n",
       "      <td>122.917943</td>\n",
       "      <td>1978.014160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weight_class    weight_alt1    weight_alt2    weight_alt3\n",
       "count  150357.000000  150357.000000  150357.000000  150357.000000\n",
       "mean        5.112770       0.492201       1.210659       8.347714\n",
       "std         8.759573       0.976010       2.497411      13.152830\n",
       "min         0.652846       0.002341       0.001517       0.748944\n",
       "25%         3.120689       0.119138       0.262486       5.195688\n",
       "50%         3.925040       0.224611       0.534652       6.439734\n",
       "75%         5.319337       0.480366       1.175415       8.684470\n",
       "max      1814.410277      50.146877     122.917943    1978.014160"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[item for item in values.columns if 'weight' in item]\n",
    "values[cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c95f79b8-c158-48f7-ad0f-c28a4d8bb735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_class</th>\n",
       "      <th>weight_alt1</th>\n",
       "      <th>weight_alt2</th>\n",
       "      <th>weight_alt3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_class</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305578</td>\n",
       "      <td>0.285824</td>\n",
       "      <td>0.936167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt1</th>\n",
       "      <td>0.305578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995229</td>\n",
       "      <td>0.343116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt2</th>\n",
       "      <td>0.285824</td>\n",
       "      <td>0.995229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.324061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt3</th>\n",
       "      <td>0.936167</td>\n",
       "      <td>0.343116</td>\n",
       "      <td>0.324061</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              weight_class  weight_alt1  weight_alt2  weight_alt3\n",
       "weight_class      1.000000     0.305578     0.285824     0.936167\n",
       "weight_alt1       0.305578     1.000000     0.995229     0.343116\n",
       "weight_alt2       0.285824     0.995229     1.000000     0.324061\n",
       "weight_alt3       0.936167     0.343116     0.324061     1.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbb62f-a5af-42c3-9401-b4bddcce4cec",
   "metadata": {},
   "source": [
    "## Value classification comparison\n",
    "Let's compare the result of the value classification with the calculation schemes _Classical_ and _Alternative1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0d55ec7-a75e-4355-89d6-dd9768722742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_ values\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Lista de raíces indoeuropeas                           \tEspecies de Rhododendron\n",
      "Telmatoscopus                                          \tLista de xentilicios de concellos galegos\n",
      "Especies de Rhododendron                               \tLista de raíces indoeuropeas\n",
      "Lista de capítulos de O detective Conan                \tBenthamia\n",
      "Isabel Soto                                            \tLista de guitarristas solistas\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Paranarrador                                           \tPartido Socialista Obrero Español en Galicia\n",
      "Búfalo anano                                           \tMidwinterhoorn\n",
      "Jamaica, Land We Love                                  \tBolsa de estudos\n",
      "Magnolia                                               \tOrdalía\n",
      "The Lincolnshire Poacher                               \tJean Calas\n",
      "\n",
      "\n",
      "\n",
      "max_ values\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Lista de raíces indoeuropeas                           \tPortal:Música rock/Efemérides destacadas/31 de marzo\n",
      "3 de xaneiro                                           \tPortal:Música rock/Efemérides destacadas/10 de decembro\n",
      "Lista de capítulos de O detective Conan                \tPortal:Aviación/Efemérides destacadas/1 de agosto\n",
      "7 de outubro                                           \t1085\n",
      "2 de xaneiro                                           \t82\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Santanyí                                               \tHockey Club Liceo da Coruña\n",
      "Herpesvíridos                                          \tCuarte de Huerva\n",
      "Lista de senadores galegos desde a Transición          \tJosé Prudencio Padilla\n",
      "The Lincolnshire Poacher                               \tPlatón\n",
      "Maelienydd                                             \tCamiño dos Faros\n",
      "\n",
      "\n",
      "\n",
      "weight_ values\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Estatuto de Autonomía de Cataluña                      \tPortal:Aviación/Efemérides destacadas/1 de agosto\n",
      "Rexión Norte                                           \tPortal:Música rock/Efemérides destacadas/31 de marzo\n",
      "Lista de capitais nacionais                            \t82\n",
      "Bomi                                                   \t1085\n",
      "Telmatoscopus                                          \tPortal:Música rock/Efemérides destacadas/10 de decembro\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Eleccións municipais de 1983 en Galicia                \tXoana de Arco\n",
      "Eleccións municipais de 2015 en Galicia                \tHolocausto\n",
      "Roseira brava                                          \tCiencia\n",
      "Eleccións municipais de 2019 en Galicia                \tHistoria de Serbia\n",
      "Magnolia                                               \tHistoria dos eslavos do sur\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pref in ['mean','max','weight']:\n",
    "    print(f'{pref}_ values')\n",
    "    res=[]\n",
    "    for suf in ['class','alt1']:\n",
    "        col=pref+'_'+suf\n",
    "        indx=values.sort_values(col).index\n",
    "        res.append((indx[:5],indx[-5:]))\n",
    "    print(f'{\"Minor values\":>40}')\n",
    "    print(f'\\t\\tClassical{\"Alternative1\":>60}')\n",
    "\n",
    "    for c,a in zip(res[0][0],res[1][0]):\n",
    "        print(f'{articles[c].title:<55}\\t{articles[a].title}')\n",
    "    print(f'\\n{\"Higher values\":>40}')\n",
    "    print(f'\\t\\tClassical{\"Alternative1\":>60}')\n",
    "    for c,a in zip(res[0][1],res[1][1]):\n",
    "        print(f'{articles[c].title:<55}\\t{articles[a].title}')\n",
    "\n",
    "    print('\\n\\n')\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcccc6-b0d2-4e42-b965-6f6eaa512114",
   "metadata": {},
   "source": [
    "And the calculation scheme with more intuitive results seems to be _weight_values_ _Alternative1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553e023-3548-4e21-84ae-7e3c459f5892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
