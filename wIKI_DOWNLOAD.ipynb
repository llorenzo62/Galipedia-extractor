{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bedcc3-21db-42b8-b8f9-879b4d85eb41",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is intended to process wikipedia dumps, with the goal of extract the maximum number of complete and significative sentences. This set of sentences could be used for any purpose in [NLP](https://en.wikipedia.org/wiki/Natural_language_processing), but information is also extracted which would be meaningful for _wikipedia_ itself.\n",
    "\n",
    "The developed example is about _Galipedia_, the galician wikipedia, but it could be easily adapted for other languages just because it is _language-agnostic_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71251b-107e-4ce1-8957-7609d244bac6",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "From __[Wikimedia Downloads](https://dumps.wikimedia.org/mirrors.html)__\n",
    "\n",
    "from the mirror _Academic Computer Club, Umeå University_ (Last 5 good XML dumps, 'other' datasets): glwiki-20221120-pages-articles.xml.bz2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603ac9db-0750-4bde-bfa2-1ae076373ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-08 22:20:53--  http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
      "Resolving ftp.acc.umu.se (ftp.acc.umu.se)... 194.71.11.173, 194.71.11.165, 194.71.11.163, ...\n",
      "Connecting to ftp.acc.umu.se (ftp.acc.umu.se)|194.71.11.173|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://gemmei.ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2 [following]\n",
      "--2022-12-08 22:20:54--  http://gemmei.ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
      "Resolving gemmei.ftp.acc.umu.se (gemmei.ftp.acc.umu.se)... 194.71.11.137, 2001:6b0:19::137\n",
      "Connecting to gemmei.ftp.acc.umu.se (gemmei.ftp.acc.umu.se)|194.71.11.137|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 302586132 (289M) [application/x-bzip2]\n",
      "Saving to: ‘glwiki-20221120-pages-articles.xml.bz2’\n",
      "\n",
      "glwiki-20221120-pag 100%[===================>] 288.57M  36.7MB/s    in 8.5s    \n",
      "\n",
      "2022-12-08 22:21:02 (34.0 MB/s) - ‘glwiki-20221120-pages-articles.xml.bz2’ saved [302586132/302586132]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ftp.acc.umu.se/mirror/wikimedia.org/dumps/glwiki/20221120/glwiki-20221120-pages-articles.xml.bz2\n",
    "\n",
    "\n",
    "!bunzip2 glwiki-20221120-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbbabb5-273e-4969-b2f5-2d8cf67ee043",
   "metadata": {},
   "source": [
    "# Libraries & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb79d118-c6d7-40ff-ae18-bc09973fe4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,os,pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from random import choice, sample\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "document=namedtuple('document',['title','category','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e26016-350a-435a-8758-fb3988ff079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(lst):\n",
    "    '''Unravel a list of lists\n",
    "    lst: a list or set or tuple of lists/sets/tuples\n",
    "    returns all values in one list'''\n",
    "    ulst=[]\n",
    "    for item in lst:\n",
    "        if type(item) in [list,set,tuple]:\n",
    "            ulst+=unravel(item)\n",
    "        else:\n",
    "            ulst.append(item)\n",
    "    return ulst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d128e365-e5a7-4eeb-b209-0724a1c3fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorice(iterable_list,thread_function,max_workers=None):\n",
    "    '''\n",
    "    given an iterable list with data, and a thread_function for process it\n",
    "    set up a Pool for vectorice the function and returns the bunches in answer.\n",
    "    max_workers stands for the number of threads launched. \n",
    "    If max_workers is None (or not an integer) it is set to the number of CPUs detected\n",
    "    '''\n",
    "    from multiprocessing.pool import Pool\n",
    "    \n",
    "    max_workers=max_workers if isinstance(max_workers,int) else os.cpu_count()\n",
    "    \n",
    "    R=lambda x,y=max_workers: list(range(0,len(x),len(x)//y))\n",
    "    \n",
    "    lR=R(iterable_list)\n",
    "    \n",
    "    params=[iterable_list[lR[i]:lR[i+1]] for i in range(len(lR)-2)]\n",
    "    params.append(iterable_list[lR[-2]:])\n",
    "    \n",
    "    pool=Pool()\n",
    "    answer=pool.map(thread_function,params)\n",
    "    del pool\n",
    "\n",
    "    return answer\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e382aec-f2b0-42fa-9bc9-3b4f585afa06",
   "metadata": {},
   "source": [
    "It could be difficult preserve the _language-agnostic_ nature for sentence tokenization. Two ways are plausible:\n",
    "* import a sentence tokenizer from nltk or any other suitable library\n",
    "* write a custom function, which can take into account especifities of the dump we work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00731fa2-ef8f-4d21-a6a4-40fad2d113d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import nltk\n",
    "#sent_tok= nltk.sent_tokenize\n",
    "\n",
    "def sent_tok(text,ends='[\\:\\?\\!\\*\\#]'):\n",
    "    if type(text)!=list:\n",
    "        text=[text]\n",
    "    res=[]\n",
    "    for item in text:\n",
    "        #preserve ellipsis\n",
    "        item=item.replace('...','…')\n",
    "        #preserve some common abreviatures\n",
    "        item=item.replace('a.C.','aC.').replace('d.C.','dC.')\n",
    "        #preserve acronyms\n",
    "        ini0=0\n",
    "        sent=''\n",
    "        for span in re.finditer('[^A-Zªº]\\. ',item):\n",
    "            ini,fin=span.span()\n",
    "            sent+=item[ini0:ini+1]+'.\\n'\n",
    "            ini0=fin\n",
    "        \n",
    "        res.append(re.sub(ends,'\\n',sent).split('\\n'))\n",
    "       \n",
    "    return ([item.strip() for item in unravel(res) if item.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16312b78-c976-4e13-be46-357343854093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_in(txt,pttrn=[':','.jpg','.png','*[']):\n",
    "    '''returns True if any of pttr is in txt'''\n",
    "    for pt in pttrn:\n",
    "        if pt in txt:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bb626-fb2d-45ac-b979-35e1b3ad8221",
   "metadata": {},
   "source": [
    "The `get_links` function try to get information to remove reference patterns while preserves the text if it is a natural part of text.  \n",
    "The `drop_rec` function removes tables and links which could be recursive and with unbalanced open and close tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a053825-ef3f-4b29-8bcd-bf285618f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page,pat_open=r'\\{\\{',pat_close=r'\\}\\}'):\n",
    "    \n",
    "   \n",
    "    \n",
    "    lini=[item.span() for item in re.finditer(pat_open,page)]\n",
    "    lfin=[item.span() for item in re.finditer(pat_close,page)]\n",
    "    if len(lfin)==0 and len(lini):\n",
    "        ini=lini[0][0] if len(lini) else 0\n",
    "        fin=len(page)-1\n",
    "        return [page[ini:fin]]\n",
    "    \n",
    "    chunks=[]\n",
    "    if len(lini)!=len(lfin):\n",
    "\n",
    "\n",
    "        indxf=0\n",
    "        indxi=0\n",
    "        while indxf<len(lfin) and indxi<len(lini):\n",
    "            ini=lini[indxi][0]\n",
    "            while indxi<len(lini) and lini[indxi][1]<lfin[indxf][0] :\n",
    "                indxi+=1\n",
    "            if indxi==len(lini):\n",
    "                fin=lfin[-1][1]\n",
    "            else:\n",
    "                while indxf<len(lfin) and lfin[indxf][0]<lini[indxi][1]:\n",
    "                    indxf+=1\n",
    "                fin=lfin[indxf-1][1]\n",
    "            chunks.append(page[ini:fin])\n",
    "    else:\n",
    "        posini=[]\n",
    "        for indx,posfin in enumerate(lfin):\n",
    "            posini+=[item for item in lini[indx:] if item[1]<posfin[0]]\n",
    "            if len([item for item in lini[indx:] if item[1]<posfin[0]])==1:\n",
    "                chunks.append(page[posini[0][0]:posfin[1]])\n",
    "                posini=[]\n",
    "            \n",
    "    \n",
    "    return sorted(chunks,key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add9807a-68fb-4c0b-841a-6cb6bb3b2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rec(page,pat_open=r'\\{\\|',pat_close=r'\\|\\}'):\n",
    "    \n",
    "    pato=re.compile(pat_open)\n",
    "    patc=re.compile(pat_close)\n",
    "\n",
    "    ini0=ini=pato.search(page,0)\n",
    "    fin0=fin=patc.search(page,0)\n",
    "    if ini0==None and fin0==None:\n",
    "        return page\n",
    "    elif fin0==None:\n",
    "        return page[:ini0.start()]\n",
    "    elif ini0==None:\n",
    "        while fin:\n",
    "            fin0=fin\n",
    "            fin=patc.search(page,fin.end())\n",
    "            \n",
    "        return page[min(fin0.end(),len(page)-1):]\n",
    " \n",
    "    chunks=[]\n",
    "    nest=0\n",
    "   \n",
    "    while fin:\n",
    "\n",
    "        while ini!=None and ini.end()<fin0.start():\n",
    "            nest+=1\n",
    "            ini=pato.search(page,ini.end())\n",
    "\n",
    "       \n",
    "        if not ini:# or nest>1:\n",
    "\n",
    "            while fin:# and nest>1:\n",
    "                nest-=1\n",
    "                fin0=fin\n",
    "                fin=patc.search(page,fin.end())\n",
    "\n",
    "        elif fin:       \n",
    "            \n",
    "            while fin and fin.start()<ini.end() :\n",
    "                nest-=1                   \n",
    "                fin0=fin\n",
    "                fin=patc.search(page,fin.end())\n",
    "\n",
    "            if fin:\n",
    "                if nest<1:\n",
    "                    chunks.append([ini0.start(),fin0.end()])\n",
    "                    ini0=ini  \n",
    "                fin0=fin\n",
    "\n",
    "        else:\n",
    "            fin=None\n",
    "        \n",
    "        nest=max(0,nest)\n",
    "\n",
    "    chunks.append([ini0.start(),fin0.end()])\n",
    "\n",
    "    if ini and ini.start()>fin0.end():\n",
    "        chunks.append([ini.start(),len(page)-1])\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    res=''\n",
    "    if chunks[0][0] == 0:                                                                                          \n",
    "        ini0=chunks[0][1]\n",
    "        chunks.pop(0)\n",
    "    else:\n",
    "        ini0=0\n",
    "    \n",
    "    for ini,fin in chunks:\n",
    "        res+=page[ini0:ini]\n",
    "        ini0=fin\n",
    "    if ini0<len(page)-1:\n",
    "        res+=page[ini0:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b15820-5ab7-4567-90f2-44b93182d145",
   "metadata": {},
   "source": [
    "## Cleaning pages\n",
    "\n",
    "The function `clean_page` is the core of this notebook. This function accepts a wiki page and process it. It returns 4 elements:\n",
    "* `title`: page title, string\n",
    "* `contributors`: list with creator username\n",
    "* `category`: list with assigned categories\n",
    "* `text`: clean text, cleaned as described below  \n",
    "\n",
    "There are a number of language dependent patterns, in this notebook the  selected patterns work with _Galipedia_, but must be easy adapt them for other languages:\n",
    "* `patt_category`: pattern to extract categories\n",
    "* `pages_to_drop`: patterns to identify in the title internal pages of wikipedia , such as 'Help', 'Model', ..., which do not contains any significant text.\n",
    "* `terminal_sections`: The wiki articles have a defined structure and there are sections located at the end of articles whithout any significant text, like 'Bibliography', 'Notes',... \n",
    "* `patt_citation`: pattern for extract textual citations and incorporate it into text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "165120b2-7775-4190-a84a-f42c793d368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "patt_category=r'\\[\\[Categoría:(.*?)\\]\\]'\n",
    "pages_todrop=['Axuda:', 'Wikipedia:','MediaWiki:','Modelo:','Categoría:','Módulo:']\n",
    "terminal_sections= ['Palmarés',  'Festividades',  'Partidos históricos.*?',  'Filmografía',  'Galería.*?',  'Notas',  'Véxase tamén',  \n",
    "                 'Bibliografía',  'Outros artigos',  'Ligazóns externas']\n",
    "patt_citation=r'\\{\\{cita ?\\|(.*?\\.)\\}\\}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "049a68ad-939d-458f-a6b0-129083d275cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page(page):\n",
    "    title=re.findall(r'<title>(.*?)</title>',page)\n",
    "    contributors=re.findall(r'<username>(.*?)</username>',page)\n",
    "    category=re.findall(patt_category,page)\n",
    "    \n",
    "    txt=re.findall(r'<text.*?>(.*?)</text>',page)\n",
    "    if title:\n",
    "        title=title[0]\n",
    "        if any_in(title,pages_todrop):\n",
    "            return '',[],[],[]\n",
    "    else:\n",
    "        return '',[],[],[]\n",
    "    if txt:\n",
    "        txt=txt[0]\n",
    "    else:\n",
    "        return '',[],[],[]\n",
    "    \n",
    "    pos=[]\n",
    "    for pat in terminal_sections:\n",
    "        pos+=[item.span()[0] for item in re.finditer(r'={2,3} {0,1}%s {0,1}={2,3}'%pat ,txt)]\n",
    "                \n",
    "    pos.sort()\n",
    "    pos=pos[0] if len(pos) else len(txt)\n",
    "    txt=txt[:pos]\n",
    "    \n",
    "    #remove latex ecuations\n",
    "    txt=re.sub(r'[<|&lt;]math.*?/math[&gt;|>]',' ',txt)\n",
    "    \n",
    "    #Remove Boxes\n",
    "    txt=re.sub(r'\\{\\{Start box\\}\\}.*?\\{\\{End box\\}\\}',' ',txt)\n",
    "    \n",
    "    #remove html divisions\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('div'),' ',txt)\n",
    "    \n",
    "    #remove graphs\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('graph'),' ',txt)\n",
    "    \n",
    "    #remove galeries\n",
    "    txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format('gallery'),' ',txt)\n",
    "\n",
    "    \n",
    "    #Remove tables, links and references, potentially recursive and unmatched\n",
    "    #remove citations\n",
    "    if '{{' in txt:\n",
    "        txt=drop_rec(txt,r'\\{\\{',r'\\}\\}')\n",
    "        \n",
    "\n",
    "    #remove tables\n",
    "    txt=txt.replace('&lt;table','&lt; {|').replace( '/table&gt;','|} &gt;')\n",
    "    txt=txt.replace('&lt;TABLE','&lt; {|').replace( '/TABLE&gt;','|} &gt;')\n",
    "    \n",
    "    if any_in(txt,[r'{|',r'|}']):\n",
    "            txt=drop_rec(txt,r'\\{\\|',r'\\|\\}')\n",
    " \n",
    "    #remove links\n",
    "    if '[[' in txt:\n",
    "        for l in get_links(txt,r'\\[\\[',r'\\]\\]'):\n",
    "\n",
    "            rpl=''\n",
    "            if not any_in(l,list(':/')):\n",
    "                m=l.strip('[]')\n",
    "                rpl=m.split('|')[1]  if '|' in l else m\n",
    "\n",
    "            txt=txt.replace(l,rpl)\n",
    "        \n",
    "   \n",
    "    #remove special cases of references\n",
    "\n",
    "    txt=re.sub(r'&lt; ?Ref.*?/ref?&gt;',' ',txt)\n",
    "   \n",
    "    #remove sections\n",
    "    \n",
    "    for tag in ['div','ref','nowiki','graph','timeline','center','Center','syntaxhighlight','sub','sup','span','time','small','big','gallery','imagemap']:\n",
    "        txt=re.sub(r'&lt; ?{0}.*?/{0}?&gt;'.format(tag),' ',txt)\n",
    "        txt=re.sub(r'&lt; ?{}.*?/&gt;'.format(tag),' ',txt)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    txt=re.sub(r'&lt;noinclude&gt;',' ',txt)\n",
    "    txt=re.sub(r'{{nowrap.*?}}',' ',txt) \n",
    "    \n",
    "    txt=re.sub(r'\\[http.*?\\]','',txt)\n",
    "    \n",
    "    txt=re.sub(r'\\{.*?\\|left','',txt)\n",
    "    txt=re.sub(r'/{0,9}center {0,20}\\|{0,9}','',txt)\n",
    "    txt=re.sub(r'nbsp;',' ',txt)\n",
    "    txt=re.sub(r'br ?/',' ',txt)\n",
    "    \n",
    "    \n",
    "    txt=re.sub(r'\\|{1,20}left','',txt)\n",
    "    txt=re.sub(r'/{1,20}math','',txt)\n",
    "    txt=re.sub(r'\\\\frac','',txt)\n",
    "   \n",
    "    \n",
    "    #rid off misswrited numbers\n",
    "\n",
    "    for p in re.findall(r'([0-9]+\\. {0,9}[0-9]+)',txt):\n",
    "\n",
    "        q=re.sub('\\. {0,9}','',p)\n",
    "        txt=re.sub(p,q,txt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Ad hoc\n",
    "    txt=re.sub(r'\\|wid.*?top\\|','',txt);\n",
    "    txt=re.sub(r'\\|\\|.*?;\\|','',txt);\n",
    "    txt=re.sub(r'\\|?rowspan.*?\\|[(Clas)|(Des)].*?[á|\\|]','',txt);\n",
    "    txt=re.sub(r'\\| ?colspan.*?[\\.|\\|]','',txt);\n",
    "    txt=re.sub(r'colspan=.*?[!|\\|]','',txt);\n",
    "    txt=re.sub(r'rowspan=.*?[!|\\|]','',txt);\n",
    "    txt=re.sub(r'\\|[\\-| ]bg.*?\\.','',txt);\n",
    "    txt=re.sub(r'\\|vh?align.*?\\|','',txt);\n",
    "    txt=re.sub(r'\\|? ?style=.*?[\\.|\\|]','',txt);\n",
    "    txt=re.sub(r'| ?align=|','',txt)\n",
    "    txt=re.sub(r'{| ?class=wikitable.*?\\|-','',txt)\n",
    "    txt=re.sub(r'{| ?class=wikitable.*?!','',txt)\n",
    "    txt=re.sub(r'\\| vtop \\| {1,2}\\| width=50% vtop \\|','',txt)\n",
    "    \n",
    "    #Tidy text for sent tokenize\n",
    "    txt=re.sub(r'etc\\.','etc…',txt)\n",
    "    txt=re.sub(r' ?={2,20} ?','. ',txt)\n",
    "    txt=re.sub(r'\\([. ]*?\\)','',txt)\n",
    "    txt=re.sub(r'\\. *?\\.','. ',txt)\n",
    "    txt=re.sub(r'\\.{2,20}','. ',txt)\n",
    "    txt=txt.replace('}',' ')\n",
    "    txt=re.sub(r'\\'{2,20}','\\'',txt)\n",
    "    txt=re.sub(r'&lt;.*?&gt;','',txt)\n",
    "    txt=re.sub(r'&.*?;','',txt)\n",
    "     \n",
    "    \n",
    "    txt=[item.strip() for item in sent_tok(txt)]\n",
    "    \n",
    "    return title, category, contributors, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021a90a0-50a4-4136-9f8e-5322d7442c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet='A-ZÁÂÉÊÍÎÓÔÚÛÜÑÇ'\n",
    "alpha_text=lambda x: re.sub('[^{} -]'.format(alphabet+alphabet.lower()),'',x)\n",
    "transpose=lambda x: list(zip(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe30afc-0649-4266-9d40-eb7b029cd580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    alphabetized=lambda x: re.sub('[^{}]'.format(alphabet+alphabet.lower()),'',x)\n",
    "    \n",
    "    if isinstance(sent,str):\n",
    "        sent=sent.split()\n",
    "    \n",
    "    sent_f=[alphabetized(i) for i in sent if alphabetized(i)]\n",
    "    if not(sent_f):\n",
    "        return None\n",
    "    sent_f=[sent_f[0] if len(sent_f[0])<2 or sent[0].istitle() else '']+[i for i in sent_f[1:] if  i.islower()] \n",
    "    sent_f=[i.lower() for i in sent_f if i]\n",
    "    return sent_f\n",
    "    \n",
    "\n",
    "def basic_feat(text):\n",
    "    \n",
    "    nsent=0\n",
    "    ntok=[]\n",
    "    nword=[]\n",
    "    sentences=[]\n",
    "    stream=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for item in text:\n",
    "        sent=[s for s in item.split() if len(s)<MAX_CHAR_TOKEN]\n",
    "        \n",
    "        if len(sent)<MIN_TOKENS :\n",
    "            continue\n",
    "        \n",
    "        sent_f=clean_text(sent)\n",
    "        if sent_f:\n",
    "            nsent+=1\n",
    "            ntok.append(len(sent))\n",
    "            nword.append(len(sent_f))\n",
    "                                                        \n",
    "            stream+=sent_f\n",
    "            sentences.append(' '.join(sent))\n",
    "        \n",
    "    return nsent,ntok,nword,Counter(stream),sentences         \n",
    "\n",
    "# \"valor\" de una frase suma de los tf-idf de los tokens de la frase (promedio por token)\n",
    "#value=lambda x: sum([tfidf[key]/len(x.split()) for key in x.split() if key in tfidf.keys()])\n",
    "\n",
    "def basic_stats(vals):\n",
    "    vals=np.array(vals)\n",
    "    if vals.size >0:\n",
    "        columns=['mean','std','min','max','Sh','Q25','Q50','Q75']\n",
    "        Sh=np.array(list(Counter(vals).values()))\n",
    "        Sh=Sh/Sh.sum()\n",
    "        res=[vals.mean(),vals.std(),vals.min(),vals.max(),-(Sh*np.log(Sh)).sum()]\n",
    "        res+=list(np.quantile(vals,[0.25,0.5,0.75]))\n",
    "        return {key:val for key,val in zip(columns,res)}\n",
    "    \n",
    "    \n",
    "    return {key:val for key,val in zip(columns,[0]*len(columns))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b57df5-50eb-431e-a549-15822d6f109e",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "* `path`: Path object pointing to xml dumps directory, this notebook and auxiliary files must be in the same folder.\n",
    "* `MIN_TOKENS`: minimum number of tokens for a valid sentence\n",
    "* `MIN_SENTS`: minumum number of valid sentences for a valid document\n",
    "* `MAX_CHAR_TOKEN`: maximum number of characters in a valid token\n",
    "* `MIN_DOCS`: minimum number of document frequency to be included in tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251e617c-c3f5-4bfb-8c09-a0639d184b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path.cwd()\n",
    "MIN_TOKENS=4\n",
    "MIN_SENTS=2\n",
    "MAX_CHAR_TOKEN=50\n",
    "MIN_DOCS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c0b944-98ef-4e13-bc81-a2655f5ced5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 9.64 s, total: 39.2 s\n",
      "Wall time: 56.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "386308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cpgl=' '.join(list(path.glob('*.xml'))[0].read_text(encoding='UTF8').split())\n",
    "\n",
    "pages=(re.findall('<page>(.*?)</page>',cpgl))\n",
    "\n",
    "with open('pages_wiki.pkl','wb') as fich:\n",
    "    pickle.dump(pages,fich)\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bb9b90-b5c0-4c42-91bd-d9202d658a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386308"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('pages_wiki.pkl','rb') as fich:\n",
    "    pages=pickle.load(fich)\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc94598-9744-4a73-9fd2-5be810b76bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(pages):\n",
    "    docs=[]\n",
    "    pgs=[]\n",
    "    for indx,page in enumerate(pages):\n",
    "        try:\n",
    "            title,category,user,text=clean_page(page)\n",
    "            #Filter internal pages\n",
    "            if not title or not text:\n",
    "                continue\n",
    "           \n",
    "            \n",
    "            #Filter documentns by length\n",
    "            test=[alpha_text(item).split() for item in text]\n",
    "            \n",
    "            test=[item for item in test if len(item)>MIN_TOKENS]\n",
    "\n",
    "            if len(test)>MIN_SENTS:\n",
    "                docs.append((title,category,user,text))\n",
    "                pgs.append(page)\n",
    "        except Exception as e:\n",
    "            print(f'Exception: {e}\\nFailure: {page}')\n",
    "           \n",
    "    return docs,pgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "405beac8-c9a6-4f40-a6fa-d70d241a9804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.98 s, sys: 2.63 s, total: 9.61 s\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "articles=[]\n",
    "selected_pages=[]\n",
    "for batch in vectorice(iterable_list=pages,  thread_function=thread_function, max_workers=1000):\n",
    "    d,p=(batch)\n",
    "    articles+=d\n",
    "    selected_pages+=p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f10fe60-28de-4032-ac64-4ab851bc7042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150261"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles20221120_gl.pkl','wb') as fich:\n",
    "    pickle.dump(articles,fich)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8471c526-f306-4311-96b9-05c4321e460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150261"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('selected_pages.pkl','wb') as fich:\n",
    "    pickle.dump(selected_pages,fich)\n",
    "len(selected_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae242e9e-483a-4e7c-974d-1defa2cbd7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150261"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('articles20221120_gl.pkl','rb') as fich:\n",
    "    articles=pickle.load(fich)\n",
    "articles=[document(*item) for item in articles]\n",
    "len(articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6d84d-f3d9-424d-847d-d638ca380f10",
   "metadata": {},
   "source": [
    "# Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "775e5b60-1d0c-4dde-8e1b-b677e1122d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 1205\n",
      "Most active users (75.0% of #articles):\n",
      "User                          \t#articles\t%articles\n",
      "------------------------------------------------------------\n",
      "InternetArchiveBot            \t   26643\t 17.731%\n",
      "Breogan2008                   \t   23128\t 15.392%\n",
      "BanjoBot 2.0                  \t   11982\t  7.974%\n",
      "Breobot                       \t    9267\t  6.167%\n",
      "Corribot                      \t    8926\t  5.940%\n",
      "HombreDHojalata               \t    6854\t  4.561%\n",
      "Chairego apc                  \t    6073\t  4.042%\n",
      "Estevoaei                     \t    6034\t  4.016%\n",
      "Zaosbot                       \t    5428\t  3.612%\n",
      "Xanetas                       \t    2843\t  1.892%\n",
      "Xas                           \t    2626\t  1.748%\n",
      "Alfonso Márquez               \t    1962\t  1.306%\n",
      "MAGHOI                        \t    1851\t  1.232%\n"
     ]
    }
   ],
   "source": [
    "users=Counter(unravel([item.user for item in articles])).most_common()\n",
    "\n",
    "limit=0.75\n",
    "print(f'Total number of users: {len(users)}')\n",
    "print(f'Most active users ({100*limit}% of #articles):')\n",
    "total=len(articles)\n",
    "cum=0\n",
    "print(f'{\"User\":<30}\\t{\"#articles\"}\\t{\"%articles\"}')\n",
    "print('-'*60)\n",
    "for user,val in users:\n",
    "    print(f'{user:<30}\\t{val:>8}\\t{val/total:>8.3%}')\n",
    "    cum+=val/total\n",
    "    if cum>limit:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde10902-0641-44cb-8be7-4b4393fad3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "If can be assumed that a user name that contains 'bot' in it identifies a bot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "32df2aa4-521a-4bbd-9b69-e6d821bc22ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of bots: 20\n",
      "Total articles: 64574,  42.975%\n",
      "number of articles by bot\n",
      "\tmean: 3228.7\n",
      "\tmin: 1\n",
      "\tQ25: 5.75\n",
      "\tQ50: 21.5\n",
      "\tQ75: 2272.75\n",
      "\tmax: 26643\n",
      "\tSh: 2.6923109941417858\n",
      "\n",
      "\n",
      "\n",
      "User name       \tnº articles \t%articles\n",
      "InternetArchiveBot   \t26643 \t\t17.731\n",
      "BanjoBot 2.0         \t11982 \t\t7.974\n",
      "Breobot              \t9267 \t\t6.167\n",
      "Corribot             \t8926 \t\t5.94\n",
      "Zaosbot              \t5428 \t\t3.612\n",
      "Chairebot            \t1221 \t\t0.813\n",
      "Aosbot               \t819 \t\t0.545\n",
      "Addbot               \t131 \t\t0.087\n",
      "BotDHojalata         \t70 \t\t0.047\n",
      "EmausBot             \t32 \t\t0.021\n",
      "Escarbot             \t11 \t\t0.007\n",
      "Xqbot                \t11 \t\t0.007\n",
      "KLBot2               \t10 \t\t0.007\n",
      "Texvc2LaTeXBot       \t9 \t\t0.006\n",
      "BanjoBot             \t7 \t\t0.005\n",
      "MGA73bot             \t2 \t\t0.001\n",
      "Prebot               \t2 \t\t0.001\n",
      "TohaomgBot           \t1 \t\t0.001\n",
      "Hector Bottai        \t1 \t\t0.001\n",
      "Jembot               \t1 \t\t0.001\n"
     ]
    }
   ],
   "source": [
    "bots={key:val for key,val in users if 'bot' in key or 'Bot' in key}\n",
    "print(f'Total number of bots: {len(bots)}')\n",
    "print(f'Total articles: {sum(bots.values())},  {100*sum(bots.values())/len(articles):0.3f}%')\n",
    "sts=basic_stats(list(bots.values()))\n",
    "print('number of articles by bot')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "    \n",
    "print('\\n\\n')\n",
    "print('User name       \\tnº articles \\t%articles')\n",
    "for key,val in Counter(bots).most_common():\n",
    "    while len(key)<20:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8bbbc772-d2c0-4419-9cb8-f1f9a8aa97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of human contributors: 1185\n",
      "number of articles by human\n",
      "\tmean: 70.98059071729958\n",
      "\tmin: 1\n",
      "\tQ25: 1.0\n",
      "\tQ50: 1.0\n",
      "\tQ75: 3.0\n",
      "\tmax: 23128\n",
      "\tSh: 1.9893721175190062\n",
      "\n",
      "\n",
      "\n",
      "Users which accounts for 90.0 % of human articles\n",
      "User name                 \tnº articles \t%articles\n",
      "Breogan2008               \t23128 \t\t15.392\n",
      "HombreDHojalata           \t6854 \t\t4.561\n",
      "Chairego apc              \t6073 \t\t4.042\n",
      "Estevoaei                 \t6034 \t\t4.016\n",
      "Xanetas                   \t2843 \t\t1.892\n",
      "Xas                       \t2626 \t\t1.748\n",
      "Alfonso Márquez           \t1962 \t\t1.306\n",
      "MAGHOI                    \t1851 \t\t1.232\n",
      "Miguelferig               \t1728 \t\t1.15\n",
      "RubenWGA                  \t1640 \t\t1.091\n",
      "Beninho                   \t1497 \t\t0.996\n",
      "Vitoriaogando             \t1323 \t\t0.88\n",
      "Moedagalega               \t1148 \t\t0.764\n",
      "Elisardojm                \t1086 \t\t0.723\n",
      "Jglamela                  \t1040 \t\t0.692\n",
      "Xosema                    \t982 \t\t0.654\n",
      "HacheDous=0               \t960 \t\t0.639\n",
      "Maria zaos                \t917 \t\t0.61\n",
      "CommonsDelinker           \t865 \t\t0.576\n",
      "Norrin strange            \t862 \t\t0.574\n",
      "One2                      \t818 \t\t0.544\n",
      "Lameiro                   \t761 \t\t0.506\n",
      "Ourol                     \t754 \t\t0.502\n",
      "Servando2                 \t732 \t\t0.487\n",
      "Gallaecio                 \t680 \t\t0.453\n",
      "Banjo                     \t648 \t\t0.431\n",
      "Tfeliz                    \t581 \t\t0.387\n",
      "Adorian                   \t567 \t\t0.377\n",
      "Xoio                      \t513 \t\t0.341\n",
      "Gasparoff                 \t503 \t\t0.335\n",
      "Lles                      \t443 \t\t0.295\n",
      "FleaRHCP                  \t382 \t\t0.254\n",
      "Ogaiago                   \t365 \t\t0.243\n",
      "Cossue                    \t363 \t\t0.242\n",
      "Alhadis                   \t362 \t\t0.241\n",
      "Mark Gasoline             \t347 \t\t0.231\n",
      "Fendetestas               \t314 \t\t0.209\n",
      "Calq                      \t312 \t\t0.208\n",
      "Ogalego.gal               \t301 \t\t0.2\n",
      "Krisko                    \t296 \t\t0.197\n",
      "Piquito                   \t292 \t\t0.194\n"
     ]
    }
   ],
   "source": [
    "human={key:val for key,val in users if not ('bot' in key or 'Bot' in key)}\n",
    "print(f'Total number of human contributors: {len(human)}')\n",
    "sts=basic_stats(list(human.values()))\n",
    "print('number of articles by human')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "\n",
    "print('\\n\\n')\n",
    "limit=0.90\n",
    "print(f'Users which accounts for {100*limit} % of human articles')\n",
    "print('User name                 \\tnº articles \\t%articles')\n",
    "acum=0\n",
    "total=sum(list(human.values()))\n",
    "for key,val in Counter(human).most_common():\n",
    "    if acum>limit:\n",
    "        break\n",
    "    acum+=val/total\n",
    "    while len(key)<25:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff859b-e771-40ea-8eaf-6ad7eb3ff037",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8ae2564-0139-464f-9cc6-05ad8fa9eee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of categories used: 69849\n",
      "number of articles by category\n",
      "\tmean: 6.983450013600767\n",
      "\tmin: 1\n",
      "\tQ25: 1.0\n",
      "\tQ50: 1.0\n",
      "\tQ75: 4.0\n",
      "\tmax: 5065\n",
      "\tSh: 2.1225501000240916\n",
      "\n",
      "\n",
      "\n",
      "Category                  \t\t\t\t\t\tnº articles \t%articles\n",
      "Personalidades de Galicia sen imaxes                                   \t5065 \t\t3.371\n",
      "Filmes en lingua inglesa                                               \t3203 \t\t2.132\n",
      "Filmes dos Estados Unidos de América                                   \t2730 \t\t1.817\n",
      "Topónimos galegos con etimoloxía                                       \t1594 \t\t1.061\n",
      "Escritores de Galicia en lingua galega                                 \t1427 \t\t0.95\n",
      "Alumnos da Universidade de Santiago de Compostela                      \t1371 \t\t0.912\n",
      "Nados en ano descoñecido                                               \t1366 \t\t0.909\n",
      "Personalidades sen imaxes                                              \t1085 \t\t0.722\n",
      "Escritores de Galicia en lingua castelá                                \t1074 \t\t0.715\n",
      "Personalidades de Galicia sen imaxes finados hai máis de 80 anos       \t994 \t\t0.662\n",
      "Xogadores de baloncesto da NBA                                         \t970 \t\t0.646\n",
      "Nados na Coruña                                                        \t968 \t\t0.644\n",
      "Alcaldes de Galicia durante a democracia                               \t855 \t\t0.569\n",
      "Xogadores de baloncesto dos Estados Unidos de América                  \t814 \t\t0.542\n",
      "Nados en Vigo                                                          \t811 \t\t0.54\n",
      "Nados en Madrid                                                        \t799 \t\t0.532\n",
      "Actores de cine dos Estados Unidos de América                          \t766 \t\t0.51\n",
      "Poetas de Galicia                                                      \t750 \t\t0.499\n",
      "Dianteiros de fútbol                                                   \t739 \t\t0.492\n",
      "Escritores en lingua inglesa                                           \t726 \t\t0.483\n",
      "Centrocampistas de fútbol                                              \t721 \t\t0.48\n",
      "Concellos do Xapón                                                     \t701 \t\t0.467\n",
      "Nados en Santiago de Compostela                                        \t650 \t\t0.433\n",
      "Escritores en lingua castelá                                           \t643 \t\t0.428\n",
      "Actores de televisión dos Estados Unidos de América                    \t624 \t\t0.415\n",
      "Asasinados polo bando nacional en Galicia                              \t624 \t\t0.415\n",
      "Satélites artificiais                                                  \t589 \t\t0.392\n",
      "Xogadores do Deportivo da Coruña                                       \t578 \t\t0.385\n",
      "Filmes en lingua castelá                                               \t570 \t\t0.379\n",
      "Defensas de fútbol                                                     \t568 \t\t0.378\n",
      "Finados en 1936                                                        \t568 \t\t0.378\n",
      "Parroquias de Galicia baixo a advocación de santa María                \t536 \t\t0.357\n",
      "Animais descritos en 1758                                              \t521 \t\t0.347\n",
      "Nados en Barcelona                                                     \t490 \t\t0.326\n",
      "Xogadores de baloncesto da ACB                                         \t490 \t\t0.326\n",
      "Filmes do Reino Unido                                                  \t477 \t\t0.317\n",
      "Finados en ano descoñecido                                             \t469 \t\t0.312\n",
      "Filmes de Francia                                                      \t458 \t\t0.305\n",
      "Nados en Pontevedra                                                    \t456 \t\t0.303\n",
      "Taxons descritos por Linné                                             \t449 \t\t0.299\n",
      "Alcaldes de Galicia durante o franquismo e a transición                \t444 \t\t0.295\n",
      "Nados en Ourense                                                       \t428 \t\t0.285\n",
      "Alcaldes de Galicia durante a Restauración                             \t428 \t\t0.285\n",
      "Alcaldes de Galicia polo PPdeG                                         \t413 \t\t0.275\n",
      "Xogadores do Celta de Vigo                                             \t407 \t\t0.271\n",
      "Exilio galego                                                          \t405 \t\t0.27\n",
      "Filmes de Warner Bros.                                                 \t396 \t\t0.264\n",
      "Represaliados polo franquismo                                          \t396 \t\t0.264\n",
      "Nados en Ferrol                                                        \t394 \t\t0.262\n",
      "Nados en París                                                         \t389 \t\t0.259\n",
      "Escritores dos Estados Unidos de América                               \t384 \t\t0.256\n",
      "Lugares de Galicia despoboados                                         \t369 \t\t0.246\n",
      "Nados en 1964                                                          \t367 \t\t0.244\n",
      "Deputados de Galicia no Congreso dos Deputados                         \t365 \t\t0.243\n",
      "Escritores de Galicia coa obra no dominio público                      \t362 \t\t0.241\n",
      "Nados en 1948                                                          \t358 \t\t0.238\n",
      "Xornalistas de Galicia                                                 \t358 \t\t0.238\n",
      "Nados en Nova York                                                     \t357 \t\t0.238\n",
      "Artigos creados co asistente para a creación de artigos                \t357 \t\t0.238\n",
      "Filmes de España                                                       \t352 \t\t0.234\n",
      "Filmes ambientados en Nova York                                        \t350 \t\t0.233\n",
      "Filmes en lingua francesa                                              \t347 \t\t0.231\n",
      "Membros correspondentes da Real Academia Galega                        \t347 \t\t0.231\n",
      "Nados en Londres                                                       \t346 \t\t0.23\n",
      "Astronautas dos Estados Unidos de América                              \t343 \t\t0.228\n",
      "Nados en 1961                                                          \t342 \t\t0.228\n",
      "Nados en 1952                                                          \t342 \t\t0.228\n",
      "Escoltas de baloncesto                                                 \t342 \t\t0.228\n",
      "Ala-pivotes de baloncesto                                              \t341 \t\t0.227\n",
      "Paseo da Fama de Hollywood                                             \t339 \t\t0.226\n",
      "Nados en 1965                                                          \t339 \t\t0.226\n",
      "Mestres de Galicia                                                     \t332 \t\t0.221\n",
      "Nados en 1956                                                          \t332 \t\t0.221\n",
      "Nados en 1959                                                          \t331 \t\t0.22\n",
      "Escritores en lingua francesa                                          \t330 \t\t0.22\n",
      "Nados en 1962                                                          \t329 \t\t0.219\n",
      "Estadounidenses de ascendencia inglesa                                 \t327 \t\t0.218\n",
      "Nados en Lugo                                                          \t327 \t\t0.218\n",
      "Nados en 1946                                                          \t327 \t\t0.218\n",
      "Plantas medicinais                                                     \t327 \t\t0.218\n",
      "Nados en 1967                                                          \t326 \t\t0.217\n",
      "Alas de baloncesto                                                     \t325 \t\t0.216\n",
      "Nados en 1958                                                          \t324 \t\t0.216\n",
      "Bases de baloncesto                                                    \t321 \t\t0.214\n",
      "Pivotes de baloncesto                                                  \t320 \t\t0.213\n",
      "Lugares de Palas de Rei                                                \t320 \t\t0.213\n",
      "Secuelas de filmes dos Estados Unidos de América                       \t319 \t\t0.212\n",
      "Alcaldes de Galicia durante a II República                             \t318 \t\t0.212\n",
      "Nados en 1966                                                          \t317 \t\t0.211\n",
      "Nados en 1949                                                          \t315 \t\t0.21\n",
      "Nados en 1947                                                          \t315 \t\t0.21\n",
      "Wikipedia:Páxinas con traducións non revisadas                         \t314 \t\t0.209\n",
      "Nados en 1951                                                          \t313 \t\t0.208\n",
      "Nados en 1954                                                          \t311 \t\t0.207\n",
      "Galegos da Arxentina                                                   \t310 \t\t0.206\n",
      "Nados en 1969                                                          \t308 \t\t0.205\n",
      "Nados en 1957                                                          \t307 \t\t0.204\n",
      "Nados en 1955                                                          \t306 \t\t0.204\n",
      "Nados en 1960                                                          \t306 \t\t0.204\n",
      "Nados en 1963                                                          \t305 \t\t0.203\n",
      "Grupos musicais de rock alternativo                                    \t304 \t\t0.202\n",
      "Nados en 1977                                                          \t304 \t\t0.202\n",
      "Nados en 1944                                                          \t304 \t\t0.202\n",
      "Nados en 1942                                                          \t303 \t\t0.202\n",
      "Nados en 1943                                                          \t301 \t\t0.2\n",
      "Artigos que toda Wikipedia debería ter (Ciencia)                       \t300 \t\t0.2\n",
      "Nados en 1968                                                          \t299 \t\t0.199\n",
      "Nados en 1971                                                          \t298 \t\t0.198\n",
      "Escritores en lingua galega                                            \t297 \t\t0.198\n",
      "Nados en 1970                                                          \t297 \t\t0.198\n",
      "Física                                                                 \t296 \t\t0.197\n",
      "Personalidades da empresa de Galicia                                   \t294 \t\t0.196\n",
      "Series de televisión dos Estados Unidos de América                     \t292 \t\t0.194\n",
      "Proteínas                                                              \t292 \t\t0.194\n",
      "Nados en 1950                                                          \t290 \t\t0.193\n",
      "Militares de Galicia                                                   \t290 \t\t0.193\n",
      "Nados en 1976                                                          \t289 \t\t0.192\n",
      "Escritores de Galicia                                                  \t289 \t\t0.192\n",
      "Nados en 1975                                                          \t289 \t\t0.192\n",
      "Pilotos das 24 Horas de Le Mans                                        \t287 \t\t0.191\n",
      "Eleccións municipais en Galicia                                        \t284 \t\t0.189\n",
      "Nados en 1953                                                          \t283 \t\t0.188\n",
      "Alumnos da Universidade Complutense de Madrid                          \t280 \t\t0.186\n",
      "Nados en 1973                                                          \t279 \t\t0.186\n",
      "Nados en 1974                                                          \t277 \t\t0.184\n",
      "Nados en 1981                                                          \t275 \t\t0.183\n",
      "Nados en 1979                                                          \t275 \t\t0.183\n",
      "Nados en 1945                                                          \t275 \t\t0.183\n",
      "Políticos do PSOE                                                      \t275 \t\t0.183\n",
      "Filmes de Universal Pictures                                           \t270 \t\t0.18\n",
      "Guerrilleiros antifranquistas de Galicia                               \t268 \t\t0.178\n",
      "Nados en 1980                                                          \t267 \t\t0.178\n",
      "Nados en 1978                                                          \t264 \t\t0.176\n",
      "Nados en 1972                                                          \t263 \t\t0.175\n",
      "Nados en 1940                                                          \t258 \t\t0.172\n",
      "Directores de cine dos Estados Unidos de América                       \t258 \t\t0.172\n",
      "Illas do Pacífico                                                      \t255 \t\t0.17\n",
      "Actores de teatro dos Estados Unidos de América                        \t254 \t\t0.169\n",
      "Alcaldes de Galicia polo PSdeG-PSOE                                    \t254 \t\t0.169\n",
      "Filmes de Columbia Pictures                                            \t254 \t\t0.169\n",
      "Nados en 1986                                                          \t252 \t\t0.168\n",
      "Lugares de Castro de Rei                                               \t252 \t\t0.168\n",
      "Finados en 1937                                                        \t250 \t\t0.166\n",
      "Lugares das Pontes de García Rodríguez                                 \t248 \t\t0.165\n",
      "Nados en 1932                                                          \t246 \t\t0.164\n",
      "Nados en 1988                                                          \t246 \t\t0.164\n",
      "Santos do catolicismo                                                  \t245 \t\t0.163\n",
      "Nados en 1928                                                          \t245 \t\t0.163\n",
      "Futbolistas gañadores da Liga de Campións da UEFA                      \t245 \t\t0.163\n",
      "Nados en 1936                                                          \t245 \t\t0.163\n",
      "Papas                                                                  \t245 \t\t0.163\n",
      "Nados en 1984                                                          \t244 \t\t0.162\n",
      "Nados en 1985                                                          \t243 \t\t0.162\n",
      "Xogadores da selección de fútbol de España                             \t241 \t\t0.16\n",
      "Nados en 1941                                                          \t241 \t\t0.16\n",
      "Nados en 1930                                                          \t240 \t\t0.16\n",
      "Filmes distribuídos en sistemas IMAX                                   \t240 \t\t0.16\n",
      "Personalidades da zooloxía                                             \t240 \t\t0.16\n",
      "Nados en 1987                                                          \t236 \t\t0.157\n",
      "Nados en 1990                                                          \t236 \t\t0.157\n",
      "Personalidades da política de Galicia                                  \t235 \t\t0.156\n",
      "Finados en 2020                                                        \t234 \t\t0.156\n",
      "Nados en 1937                                                          \t232 \t\t0.154\n",
      "Filmes de 20th Century Fox                                             \t231 \t\t0.154\n",
      "Filmes rodados nos Ánxeles                                             \t231 \t\t0.154\n",
      "Nados en 1989                                                          \t231 \t\t0.154\n",
      "Finados en 2021                                                        \t230 \t\t0.153\n",
      "Grupos musicais de indie rock                                          \t230 \t\t0.153\n",
      "Ministros de España                                                    \t229 \t\t0.152\n",
      "Nados en 1982                                                          \t228 \t\t0.152\n",
      "Parroquias de Galicia baixo a advocación de san Pedro                  \t226 \t\t0.15\n",
      "Profesores da Universidade de Santiago de Compostela                   \t226 \t\t0.15\n",
      "Plantas descritas en 1753                                              \t223 \t\t0.148\n",
      "Nados en 1934                                                          \t222 \t\t0.148\n",
      "Lugares de Carballedo                                                  \t221 \t\t0.147\n",
      "Galegos de Cuba                                                        \t220 \t\t0.146\n",
      "Termos zoolóxicos                                                      \t219 \t\t0.146\n",
      "Escritores en lingua portuguesa                                        \t219 \t\t0.146\n",
      "Filmes de Alemaña                                                      \t219 \t\t0.146\n",
      "Termos botánicos                                                       \t218 \t\t0.145\n",
      "Nados en 1935                                                          \t218 \t\t0.145\n",
      "Nados en 1939                                                          \t218 \t\t0.145\n",
      "Nados en 1931                                                          \t217 \t\t0.144\n",
      "Personalidades da música de Galicia                                    \t216 \t\t0.144\n",
      "Gardametas de fútbol                                                   \t216 \t\t0.144\n",
      "Nados en 1933                                                          \t216 \t\t0.144\n",
      "Filmes preservados no National Film Registry                           \t215 \t\t0.143\n",
      "Filmes de Paramount Pictures                                           \t215 \t\t0.143\n",
      "Profesores de educación secundaria                                     \t214 \t\t0.142\n",
      "Escritores de Francia                                                  \t213 \t\t0.142\n",
      "Premios Nobel dos Estados Unidos de América                            \t212 \t\t0.141\n",
      "Lugares de Ortigueira                                                  \t212 \t\t0.141\n",
      "Nados en 1991                                                          \t211 \t\t0.14\n",
      "Artigos de Galicia sen imaxes                                          \t210 \t\t0.14\n",
      "Parroquias de Galicia baixo a advocación de Santiago o Maior           \t209 \t\t0.139\n",
      "Finados en 2012                                                        \t208 \t\t0.138\n",
      "Nados en Buenos Aires                                                  \t208 \t\t0.138\n",
      "Moedas fóra de curso                                                   \t207 \t\t0.138\n",
      "Artigos destacados                                                     \t206 \t\t0.137\n",
      "Membros da Royal Society                                               \t206 \t\t0.137\n",
      "Nados en 1926                                                          \t206 \t\t0.137\n",
      "Finados en 2018                                                        \t205 \t\t0.136\n",
      "Xogadores do Club Baloncesto Breogán                                   \t205 \t\t0.136\n",
      "Artigos que toda Wikipedia debería ter (Biografías)                    \t204 \t\t0.136\n",
      "Nados en 1983                                                          \t204 \t\t0.136\n",
      "Actores de voz dos Estados Unidos de América                           \t202 \t\t0.134\n",
      "Nados en 1921                                                          \t202 \t\t0.134\n",
      "Empresas de Galicia                                                    \t201 \t\t0.134\n",
      "Nados en 1908                                                          \t200 \t\t0.133\n"
     ]
    }
   ],
   "source": [
    "categories=unravel([item.category for item in articles])\n",
    "categories=Counter(categories).most_common()\n",
    "print(f'Total number of categories used: {len(categories)}')\n",
    "sts=basic_stats(list(dict(categories).values()))\n",
    "print('number of articles by category')\n",
    "for key in ['mean','min','Q25','Q50','Q75','max','Sh']:\n",
    "    print(f'\\t{key}: {sts[key]}')\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "print('Category                  \\t\\t\\t\\t\\t\\tnº articles \\t%articles')\n",
    "for key,val in categories:\n",
    "    if val<200:\n",
    "        break\n",
    "    while len(key)<70:\n",
    "        key+=' '\n",
    "    print(f'{key} \\t{val} \\t\\t{round(100*val/len(articles),3)}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e8573-c6f5-4720-bc20-59efa16f3c6a",
   "metadata": {},
   "source": [
    "# Basic features  \n",
    "Language-agnostic preliminary analysis, so there is not misspelled words control or token lemmatization.  \n",
    "It relies on three routines:  \n",
    "* `clean_text`: Used to create the _Bag of Words_ (bow) for each article. Only alphabetical chars are allowed. The input is a sentence (list of tokens or string) and the output is a list of alphabetical tokens.\n",
    "* `basic_feat`: the main routine. The input is the extracted text of each article. This is processed to get:\n",
    "    * The number of sentences in the article, an `int`\n",
    "    * A list with the number of tokens in each sentence of the document\n",
    "    * A list with the number of tokens in the output of `clean_text` applied to each sentence; these are called _words_.\n",
    "    * A dictionary with the bow of the document, as defined above.\n",
    "    * A list with the sentences of the article text, after applied the `MAX_CHAR_TOKEN` filter and the `MIN_TOKENS` filter\n",
    "* `basic_stats`: the input is a list of numerical values and returns a dictionary with the mean, standard deviation, maximum, minimun, informational entropy and th quantile values for 25%, 50% (median) and 75%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "807be7e8-3573-4248-b675-2ec7e9b33ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_function(arts):\n",
    "    bows=[]\n",
    "    feats=[]\n",
    "    sents=[]\n",
    "\n",
    "    for art in arts:\n",
    "        nsent,ntok,nword,bow,sentences=basic_feat(art.text)\n",
    "        sents.append(sentences)\n",
    "        bows.append(bow)\n",
    "        numt=basic_stats(ntok)\n",
    "        numw=basic_stats(nword)\n",
    "        nums=basic_stats(list(bow.values()))\n",
    "        feats.append([art.title,nsent,numt['mean'],numt['Sh'],numw['mean'],numw['Sh'],nums['Sh'],sum(list(bow.values()))/len(bow) if bow else 0])\n",
    "    return (bows,feats,sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4325a456-3886-4af9-ac18-82e8607fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 2.72 s, total: 14.2 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "answer=vectorice(articles,thread_function,max_workers=256)\n",
    "\n",
    "bows=[]\n",
    "feats=[]\n",
    "sents=[]\n",
    "\n",
    "for b,f,s in answer:\n",
    "    bows+=(b)\n",
    "    feats+=(f)\n",
    "    sents+=(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0726f-129d-4950-86f9-999e3c7f1c4f",
   "metadata": {},
   "source": [
    "# Basic features\n",
    "Data frame with basic features of each article:\n",
    "* `key`: article title\n",
    "* `nsent`: number of sentences in the article\n",
    "* `mean_tok`: mean of number of tokens per sentence in article\n",
    "* `Sh_tok`: Informational entropy of tokens per sentence in article, in _nats_\n",
    "* `mean_word`: mean of number of alphabetical tokens (`clean_text` output) per sentence in article\n",
    "* `Sh_word`: Informational entropy of alphabetical tokens per sentence in article\n",
    "* `Sh_bow`: Informational entropy of article's Bag of Words (bow) \n",
    "* `IL`: Lexical index, defined as $\\cfrac{\\#(words~in~article)}{\\#(unique~words)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cc98a50-f71a-4d12-951a-55ed6a73bfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsent</th>\n",
       "      <th>mean_tok</th>\n",
       "      <th>Sh_tok</th>\n",
       "      <th>mean_word</th>\n",
       "      <th>Sh_word</th>\n",
       "      <th>Sh_bow</th>\n",
       "      <th>IL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.703110</td>\n",
       "      <td>19.160715</td>\n",
       "      <td>1.999124</td>\n",
       "      <td>15.365211</td>\n",
       "      <td>1.951485</td>\n",
       "      <td>0.867772</td>\n",
       "      <td>1.755180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>33.389359</td>\n",
       "      <td>6.285748</td>\n",
       "      <td>0.705631</td>\n",
       "      <td>5.843242</td>\n",
       "      <td>0.704695</td>\n",
       "      <td>0.239403</td>\n",
       "      <td>0.954831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.194444</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.122449</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.823529</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.715393</td>\n",
       "      <td>1.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>1.906155</td>\n",
       "      <td>0.868114</td>\n",
       "      <td>1.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.875000</td>\n",
       "      <td>2.516410</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.466577</td>\n",
       "      <td>1.016376</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1507.000000</td>\n",
       "      <td>101.500000</td>\n",
       "      <td>4.283692</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>4.145550</td>\n",
       "      <td>2.516510</td>\n",
       "      <td>96.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               nsent       mean_tok         Sh_tok      mean_word  \\\n",
       "count  150261.000000  150261.000000  150261.000000  150261.000000   \n",
       "mean       18.703110      19.160715       1.999124      15.365211   \n",
       "std        33.389359       6.285748       0.705631       5.843242   \n",
       "min         2.000000       4.194444      -0.000000       1.122449   \n",
       "25%         5.000000      14.823529       1.386294      11.250000   \n",
       "50%         9.000000      19.000000       1.945910      15.200000   \n",
       "75%        19.000000      22.875000       2.516410      19.000000   \n",
       "max      1507.000000     101.500000       4.283692      72.500000   \n",
       "\n",
       "             Sh_word         Sh_bow             IL  \n",
       "count  150261.000000  150261.000000  150261.000000  \n",
       "mean        1.951485       0.867772       1.755180  \n",
       "std         0.704695       0.239403       0.954831  \n",
       "min        -0.000000      -0.000000       1.000000  \n",
       "25%         1.386294       0.715393       1.423077  \n",
       "50%         1.906155       0.868114       1.628571  \n",
       "75%         2.466577       1.016376       1.900000  \n",
       "max         4.145550       2.516510      96.608696  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats=pd.DataFrame(feats,columns=[ 'key','nsent','mean_tok','Sh_tok','mean_word','Sh_word','Sh_bow','IL'])\n",
    "feats.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a40672e5-0d90-44d4-a152-4fc6474b4737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>nsent</th>\n",
       "      <th>mean_tok</th>\n",
       "      <th>Sh_tok</th>\n",
       "      <th>mean_word</th>\n",
       "      <th>Sh_word</th>\n",
       "      <th>Sh_bow</th>\n",
       "      <th>IL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47255</th>\n",
       "      <td>José María Balmón</td>\n",
       "      <td>2</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.198515</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47504</th>\n",
       "      <td>Edward Santana</td>\n",
       "      <td>2</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.456334</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70458</th>\n",
       "      <td>Edovius</td>\n",
       "      <td>2</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.545869</td>\n",
       "      <td>1.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60988</th>\n",
       "      <td>Sterling Beaumon</td>\n",
       "      <td>2</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.470236</td>\n",
       "      <td>1.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106703</th>\n",
       "      <td>Bolidophyceae</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.286836</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64604</th>\n",
       "      <td>Historia de Serbia</td>\n",
       "      <td>967</td>\n",
       "      <td>23.766287</td>\n",
       "      <td>3.845294</td>\n",
       "      <td>20.478800</td>\n",
       "      <td>3.732687</td>\n",
       "      <td>1.500926</td>\n",
       "      <td>4.462145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ciencia</td>\n",
       "      <td>1023</td>\n",
       "      <td>21.548387</td>\n",
       "      <td>3.764184</td>\n",
       "      <td>20.116325</td>\n",
       "      <td>3.710465</td>\n",
       "      <td>1.564327</td>\n",
       "      <td>4.766968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93537</th>\n",
       "      <td>Eleccións municipais de 2015 en Galicia</td>\n",
       "      <td>1312</td>\n",
       "      <td>8.362043</td>\n",
       "      <td>2.184080</td>\n",
       "      <td>3.057927</td>\n",
       "      <td>1.573469</td>\n",
       "      <td>1.857073</td>\n",
       "      <td>10.872629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106532</th>\n",
       "      <td>Mártires do século XX en España</td>\n",
       "      <td>1346</td>\n",
       "      <td>5.564636</td>\n",
       "      <td>1.622353</td>\n",
       "      <td>1.943536</td>\n",
       "      <td>1.237220</td>\n",
       "      <td>1.608374</td>\n",
       "      <td>4.158983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62153</th>\n",
       "      <td>Historia dos eslavos do sur</td>\n",
       "      <td>1507</td>\n",
       "      <td>25.094891</td>\n",
       "      <td>3.902138</td>\n",
       "      <td>21.678832</td>\n",
       "      <td>3.799351</td>\n",
       "      <td>1.654813</td>\n",
       "      <td>5.349599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150261 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            key  nsent   mean_tok    Sh_tok  \\\n",
       "47255                         José María Balmón      2  16.500000  0.693147   \n",
       "47504                            Edward Santana      2  20.000000  0.693147   \n",
       "70458                                   Edovius      2  22.500000  0.693147   \n",
       "60988                          Sterling Beaumon      2  18.500000  0.693147   \n",
       "106703                            Bolidophyceae      3   9.000000  1.098612   \n",
       "...                                         ...    ...        ...       ...   \n",
       "64604                        Historia de Serbia    967  23.766287  3.845294   \n",
       "39                                      Ciencia   1023  21.548387  3.764184   \n",
       "93537   Eleccións municipais de 2015 en Galicia   1312   8.362043  2.184080   \n",
       "106532          Mártires do século XX en España   1346   5.564636  1.622353   \n",
       "62153               Historia dos eslavos do sur   1507  25.094891  3.902138   \n",
       "\n",
       "        mean_word   Sh_word    Sh_bow         IL  \n",
       "47255   12.500000  0.693147  0.198515   1.250000  \n",
       "47504   15.000000  0.693147  0.456334   1.250000  \n",
       "70458   21.500000  0.693147  0.545869   1.228571  \n",
       "60988   13.500000  0.693147  0.470236   1.173913  \n",
       "106703   8.666667  1.098612  0.286836   1.083333  \n",
       "...           ...       ...       ...        ...  \n",
       "64604   20.478800  3.732687  1.500926   4.462145  \n",
       "39      20.116325  3.710465  1.564327   4.766968  \n",
       "93537    3.057927  1.573469  1.857073  10.872629  \n",
       "106532   1.943536  1.237220  1.608374   4.158983  \n",
       "62153   21.678832  3.799351  1.654813   5.349599  \n",
       "\n",
       "[150261 rows x 8 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.sort_values('nsent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de8b43-f9a0-4347-b7a5-fa2a18b311ec",
   "metadata": {},
   "source": [
    "# Computing idf\n",
    "\n",
    "[_idf_](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) has a yet long history in information retrieval and gives a way to estimate the relative _value_ of a word, a sentence or a document.\n",
    " The classical definition is $\\mathrm{idf}(t, D) = \\log \\cfrac{N}{|\\{d \\in D: t \\in d\\}|}$; where $N$ is the number of documents, $D$ in the collection (in this context the number of extracted articles), and $|\\{d \\in D: t \\in d\\}|$ the number of documents, $d$, which contains the term $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c88bf-e168-4a2b-975a-a57a6d9f307b",
   "metadata": {},
   "source": [
    "The function `get_tfidf_bows` gets a list with the [_Bag of Wods_](https://en.wikipedia.org/wiki/Bag-of-words_model) for each document as input and returns:\n",
    "* `ndw` the _idf_ for each term in the collection. \n",
    "* `bow` the _Bag of Words_ for the entire collection. A dictionary where the term is the key. The keys in ndw and bow are the same\n",
    "* `tfidf` the _tfidf_ value for each document in the collection: _bow·ndw_\n",
    "\n",
    "All three are python dictionaries where the term is the key. \n",
    "Applying `tfidf` or `ndw` implies that any term not included in these dictionaries has 0 value.\n",
    "\n",
    "Three filters could be applied to construct `nwd` and `tfidf`:\n",
    "* `min_len` : minimum length of the term, so word of 1, 2 or 3 letters have a low probability of be significant.\n",
    "* `min_docs` : minimum number of documents that include the term for the term to be included. It is supposed that this filter removes misspelled words and extremely exotic words.\n",
    "* `todrop` : a set with terms to be excluded arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6b6e728-df79-4ac0-a128-a8a1d18c885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_bows(bows,todrop={},min_len=4,min_docs=MIN_DOCS):\n",
    "    \n",
    "    bow={}\n",
    "    ndw={}\n",
    "    for b in bows:\n",
    "        for key,val in b.items():\n",
    "            if len(key)<min_len or key in todrop:\n",
    "                continue\n",
    "            bow[key]=bow.get(key,0)+val\n",
    "            ndw[key]=ndw.get(key,0)+1\n",
    "        \n",
    "    tfidf={}\n",
    "    ndw={key:val for key,val in ndw.items() if val>min_docs}\n",
    "    total=len(bows)\n",
    "    ndw={key:np.log(total/val) for key,val in ndw.items()}\n",
    "    bow={key:bow[key] for key in ndw.keys()}\n",
    "    total=sum(bow.values())\n",
    "    bow={key:val/total for key,val in bow.items()}\n",
    "    tfidf={key:val*bow[key] for key,val in ndw.items()}\n",
    "    return tfidf,bow,ndw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4946b67-953b-43c2-9c03-d1af167422f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.06 s, sys: 21.4 ms, total: 7.08 s\n",
      "Wall time: 7.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf,BOW,ndw=get_tfidf_bows(bows,todrop={},min_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00969be4-7cd9-47bb-bb3c-c7d5eaa1dd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('autotróficos', 10.533834699912223),\n",
       " ('sostelos', 10.533834699912223),\n",
       " ('casiodoro', 10.533834699912223),\n",
       " ('aedificatoria', 10.533834699912223),\n",
       " ('alberti', 10.533834699912223),\n",
       " ('baudelaire', 10.533834699912223),\n",
       " ('reprodutibilidade', 10.533834699912223),\n",
       " ('reinterpretan', 10.533834699912223),\n",
       " ('imitativas', 10.533834699912223),\n",
       " ('trivio', 10.533834699912223),\n",
       " ('poesis', 10.533834699912223),\n",
       " ('valorable', 10.533834699912223),\n",
       " ('aprecialas', 10.533834699912223),\n",
       " ('ordenalas', 10.533834699912223),\n",
       " ('fixalas', 10.533834699912223)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ndw).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17924964-2462-4a8f-9da8-f7c26cdca2fc",
   "metadata": {},
   "source": [
    "## tfidf sentences based\n",
    "The goal of this work is sentence-based, so it seems plausible compute the idf over an all sentences basis, that is think each sentence as a document for idf calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ff9e7e3-6237-45b6-8ac0-b0ec2cfdd58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2744914"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only sentences with more than 3 tokens\n",
    "sentences=[item for item in unravel(sents) if len(item.split())>3]\n",
    "#only unique sentences\n",
    "sentences=list(set(sentences))\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30d56f9c-4ab8-4b25-8a7d-a6379e95fa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 0 ns, total: 2min 17s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_tfidf={}\n",
    "all_bow={}\n",
    "all_ndw={}\n",
    "for s in sentences:\n",
    "    tbow=Counter(clean_text(s))\n",
    "    for key,val in tbow.items():\n",
    "        all_bow[key]=all_bow.get(key,0)+val\n",
    "        all_ndw[key]=all_ndw.get(key,0)+1\n",
    "all_ndw={key:val for key,val in all_ndw.items() if val > MIN_DOCS and len(key)>2}\n",
    "all_bow={key:val for key,val in all_bow.items() if key in all_ndw.keys()}\n",
    "total=sum(list(all_bow.values()))\n",
    "all_bow={key:val/total for key,val in all_bow.items()}\n",
    "total=len(sentences)\n",
    "all_ndw={key:np.log(total/val) for key,val in all_ndw.items()}\n",
    "all_tfidf={key:val*all_bow[key] for key,val in all_ndw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "003ed22d-0ed5-4715-94fd-ed1567f55911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('desvalorizarse', 13.438965941624746),\n",
       " ('desestiba', 13.438965941624746),\n",
       " ('precompresión', 13.438965941624746),\n",
       " ('bedeis', 13.438965941624746),\n",
       " ('acaramelado', 13.438965941624746),\n",
       " ('bicoca', 13.438965941624746),\n",
       " ('tomounas', 13.438965941624746),\n",
       " ('doñana', 13.438965941624746),\n",
       " ('penalizando', 13.438965941624746),\n",
       " ('damours', 13.438965941624746),\n",
       " ('attenboroughi', 13.438965941624746),\n",
       " ('contribuiço', 13.438965941624746),\n",
       " ('serpentean', 13.438965941624746),\n",
       " ('ferroquelatase', 13.438965941624746),\n",
       " ('fiscalizadora', 13.438965941624746)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_ndw).most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449347a6-4a9e-4ca3-8ee6-65027c4d17bd",
   "metadata": {},
   "source": [
    "## value computation\n",
    "\n",
    "So, there is four plausible computation schemas:\n",
    "* Classical: $tfidf=ndw \\cdot bow$ where $ndw$ is the _idf_ on a document basis and $bow$ the bag of words of the document \n",
    "* Alternative1: $tfidf=ndw \\cdot BOW$ where $BOW$ is the _Bag of Words_ for the collection of documents. These $tfidf$ values are the same for all documents.\n",
    "* Alternative2: the same computation that _alternative1_, but $ndw$ is computed over sentences basis, as explain above\n",
    "* Alternative3: the same computation that _classical_,  but $ndw$ is computed over sentences basis, as explain above\n",
    "\n",
    "Values per sentence are computated with the function `get_value`:\n",
    "* `sent`: the sentence, as list of tokens or string\n",
    "* `tfidf`: the tfidf to apply for computation\n",
    "* `func`: the function to apply to get the sentence value, `sum` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fa417b1-899a-4938-9f02-c71afb5a01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(sent,tfidf,func=np.sum):\n",
    "    if isinstance(sent,str):\n",
    "        sent=sent.split()\n",
    "    res=[tfidf[key] for key in clean_text(sent) if key in tfidf.keys()]\n",
    "    return func(res) if res else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fa664-6cbc-4a5e-a3ad-4f01e4cbd45a",
   "metadata": {},
   "source": [
    "For easy comparison, values results are recorded in a dataframe. Columns are named with a prefix and a suffix. The suffix is related to the method of computation: _Classical_ ==> _class_; _Alternative1_ ==> _alt1_ and so on.\n",
    "The prefix are:\n",
    "* _mean_: mean sentence value of sentences in article\n",
    "* _max_: maximun sentence value in article\n",
    "* _Sh_ : informational entropy of sentences values\n",
    "* _weight_ : summation of sentences values in article weighted by $\\left( 1+\\cfrac{1}{\\#sentences} \\right)$, so the effect of article length is somehow moderated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20834ab7-c5be-40c6-82a1-fe5f72af6b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "454bdda2-8743-4c36-a587-768c170d8dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 10.7 ms, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Classical\n",
    "v=[]\n",
    "for sent,bow in zip(sents,bows):\n",
    "    total=sum(list(bow.values()))\n",
    "    td={key:val*ndw[key]/total for key,val in bow.items() if key in ndw.keys()}\n",
    "    z=[get_value(s,td) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "\n",
    "values=pd.DataFrame(v,columns=['mean_class','max_class','Sh_class','weight_class'])\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95c898dd-8e8e-4ad6-9de8-51b0bbf55686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 34.1 ms, total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative1\n",
    "v=[]\n",
    "for sent in sents:\n",
    "    z=[get_value(s,tfidf) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt1'],values['max_alt1'],values['Sh_alt1'],values['weight_alt1']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03f2ba0a-468c-4afc-bd8c-6f2a0baf425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 3.11 ms, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative2\n",
    "v=[]\n",
    "for sent in sents:\n",
    "    z=[get_value(s,all_tfidf) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt2'],values['max_alt2'],values['Sh_alt2'],values['weight_alt2']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a78b1b6-af71-4d81-a56b-e5338c3b6c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 48s, sys: 15.2 ms, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Alternative3\n",
    "v=[]\n",
    "for sent,bow in zip(sents,bows):\n",
    "    total=sum(list(bow.values()))\n",
    "    td={key:val*all_ndw[key]/total for key,val in bow.items() if key in all_ndw.keys()}\n",
    "    z=[get_value(s,td) for s in sent]\n",
    "    val=basic_stats(z)\n",
    "    w=1+1/len(z)\n",
    "    v.append((val['mean'],val['max'],val['Sh'],sum(z)*w))\n",
    "\n",
    "\n",
    "values['mean_alt3'],values['max_alt3'],values['Sh_alt3'],values['weight_alt3']=transpose(v)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a10a824-0ac2-4f27-9ed2-634f04c1a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_class</th>\n",
       "      <th>weight_alt1</th>\n",
       "      <th>weight_alt2</th>\n",
       "      <th>weight_alt3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "      <td>150261.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.094203</td>\n",
       "      <td>0.491009</td>\n",
       "      <td>1.206788</td>\n",
       "      <td>8.322778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.208282</td>\n",
       "      <td>0.968784</td>\n",
       "      <td>2.476938</td>\n",
       "      <td>11.949575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.653132</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.748882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.121288</td>\n",
       "      <td>0.119291</td>\n",
       "      <td>0.262656</td>\n",
       "      <td>5.194379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.925353</td>\n",
       "      <td>0.224685</td>\n",
       "      <td>0.534423</td>\n",
       "      <td>6.438380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.317236</td>\n",
       "      <td>0.479906</td>\n",
       "      <td>1.173499</td>\n",
       "      <td>8.679648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>812.725555</td>\n",
       "      <td>50.177468</td>\n",
       "      <td>122.930273</td>\n",
       "      <td>1322.069086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        weight_class    weight_alt1    weight_alt2    weight_alt3\n",
       "count  150261.000000  150261.000000  150261.000000  150261.000000\n",
       "mean        5.094203       0.491009       1.206788       8.322778\n",
       "std         7.208282       0.968784       2.476938      11.949575\n",
       "min         0.653132       0.002347       0.001522       0.748882\n",
       "25%         3.121288       0.119291       0.262656       5.194379\n",
       "50%         3.925353       0.224685       0.534423       6.438380\n",
       "75%         5.317236       0.479906       1.173499       8.679648\n",
       "max       812.725555      50.177468     122.930273    1322.069086"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[item for item in values.columns if 'weight' in item]\n",
    "values[cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a2528-ebfa-45b1-ace7-ba23eee850b1",
   "metadata": {},
   "source": [
    "As expected there is high correlation between _Classical_ and _Alternative3_ by one hand and between alternatives _1_ and _2_   for all computed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c95f79b8-c158-48f7-ad0f-c28a4d8bb735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_class</th>\n",
       "      <th>weight_alt1</th>\n",
       "      <th>weight_alt2</th>\n",
       "      <th>weight_alt3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_class</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.369648</td>\n",
       "      <td>0.345998</td>\n",
       "      <td>0.935061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt1</th>\n",
       "      <td>0.369648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995196</td>\n",
       "      <td>0.375781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt2</th>\n",
       "      <td>0.345998</td>\n",
       "      <td>0.995196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_alt3</th>\n",
       "      <td>0.935061</td>\n",
       "      <td>0.375781</td>\n",
       "      <td>0.354982</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              weight_class  weight_alt1  weight_alt2  weight_alt3\n",
       "weight_class      1.000000     0.369648     0.345998     0.935061\n",
       "weight_alt1       0.369648     1.000000     0.995196     0.375781\n",
       "weight_alt2       0.345998     0.995196     1.000000     0.354982\n",
       "weight_alt3       0.935061     0.375781     0.354982     1.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbb62f-a5af-42c3-9401-b4bddcce4cec",
   "metadata": {},
   "source": [
    "## Value classification comparison\n",
    "Let's compare the result of the value classification with the calculation schemes _Classical_ and _Alternative1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f0d55ec7-a75e-4355-89d6-dd9768722742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN_ values\n",
      "-----------------------------------------------------------------\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Telmatoscopus                                          \tEspecies de Rhododendron\n",
      "Especies de Rhododendron                               \tLista de xentilicios de concellos galegos\n",
      "Lista de raíces indoeuropeas                           \tLista de guitarristas solistas\n",
      "Lista de capítulos de O detective Conan                \tBenthamia\n",
      "Isabel Soto                                            \tCadros de xogadores de tempadas pasadas do Hockey Club Liceo\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Hisham I                                               \tPartido Socialista Obrero Español en Galicia\n",
      "Paranarrador                                           \tMidwinterhoorn\n",
      "Búfalo anano                                           \tBolsa de estudos\n",
      "Jamaica, Land We Love                                  \tOrdalía\n",
      "The Lincolnshire Poacher                               \tJean Calas\n",
      "\n",
      "\n",
      "\n",
      "MAX_ values\n",
      "-----------------------------------------------------------------\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Lista de raíces indoeuropeas                           \tPortal:Música rock/Efemérides destacadas/31 de marzo\n",
      "3 de xaneiro                                           \tPortal:Música rock/Efemérides destacadas/10 de decembro\n",
      "Lista de capítulos de O detective Conan                \tPortal:Aviación/Efemérides destacadas/1 de agosto\n",
      "7 de outubro                                           \t1085\n",
      "2 de xaneiro                                           \t82\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Santanyí                                               \tHockey Club Liceo da Coruña\n",
      "Herpesvíridos                                          \tCuarte de Huerva\n",
      "Festival de Eurovisión Júnior 2008                     \tJosé Prudencio Padilla\n",
      "Lista de senadores galegos desde a Transición          \tPlatón\n",
      "The Lincolnshire Poacher                               \tCamiño dos Faros\n",
      "\n",
      "\n",
      "\n",
      "WEIGHT_ values\n",
      "-----------------------------------------------------------------\n",
      "                            Minor values\n",
      "\t\tClassical                                                Alternative1\n",
      "Estatuto de Autonomía de Cataluña                      \tPortal:Aviación/Efemérides destacadas/1 de agosto\n",
      "Rexión Norte                                           \tPortal:Música rock/Efemérides destacadas/31 de marzo\n",
      "Lista de capitais nacionais                            \t82\n",
      "Bomi                                                   \t1085\n",
      "Telmatoscopus                                          \tPortal:Música rock/Efemérides destacadas/10 de decembro\n",
      "\n",
      "                           Higher values\n",
      "\t\tClassical                                                Alternative1\n",
      "Eleccións municipais de 1995 en Galicia                \tXoana de Arco\n",
      "Eleccións municipais de 1987 en Galicia                \tHistoria de Serbia\n",
      "Eleccións municipais de 1983 en Galicia                \tHolocausto\n",
      "Eleccións municipais de 2015 en Galicia                \tCiencia\n",
      "Eleccións municipais de 2019 en Galicia                \tHistoria dos eslavos do sur\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pref in ['mean','max','weight']:\n",
    "    print(f'{pref.upper()}_ values')\n",
    "    print('-'*65)\n",
    "    res=[]\n",
    "    for suf in ['class','alt1']:\n",
    "        col=pref+'_'+suf\n",
    "        indx=values.sort_values(col).index\n",
    "        res.append((indx[:5],indx[-5:]))\n",
    "    print(f'{\"Minor values\":>40}')\n",
    "    print(f'\\t\\tClassical{\"Alternative1\":>60}')\n",
    "\n",
    "    for c,a in zip(res[0][0],res[1][0]):\n",
    "        print(f'{articles[c].title:<55}\\t{articles[a].title}')\n",
    "    print(f'\\n{\"Higher values\":>40}')\n",
    "    print(f'\\t\\tClassical{\"Alternative1\":>60}')\n",
    "    for c,a in zip(res[0][1],res[1][1]):\n",
    "        print(f'{articles[c].title:<55}\\t{articles[a].title}')\n",
    "\n",
    "    print('\\n\\n')\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebcccc6-b0d2-4e42-b965-6f6eaa512114",
   "metadata": {},
   "source": [
    "And the calculation scheme with more intuitive results seems to be _weight_values_ _Alternative1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553e023-3548-4e21-84ae-7e3c459f5892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
